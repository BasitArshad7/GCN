{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#text = (\"After Abraham Lincoln won the November 1860 presidential \"\n",
    "#        \"election on an anti-slavery platform, an initial seven \"\n",
    "#        \"slave states declared their secession from the country \"\n",
    "#        \"to form the Confederacy. War broke out in April 1861 \"\n",
    "#        \"when secessionist forces attacked Fort Sumter in South \"\n",
    "#        \"Carolina, just over a month after Lincoln's \"\n",
    "#        \"inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats in equal dimension to input_ids\n",
    "#rand = torch.rand(inputs.input_ids.shape)\n",
    "# where the random array is less than 0.15, we set true\n",
    "#mask_arr = rand < 0.15\n",
    "#mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(inputs.input_ids != 101) * (inputs.input_ids != 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\n",
    "#mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selection from mask_arr\n",
    "#selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
    "#selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply selection index to inputs.input_ids, adding MASK tokens\n",
    "#inputs.input_ids[0, selection] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for torch.\n",
    "\"\"\"\n",
    "\n",
    "import click, ast, torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### torch specific functions\n",
    "def get_optimizer(name, parameters, lr, l2=0):\n",
    "    if name == 'sgd':\n",
    "        return torch.optim.SGD(parameters, lr=lr, weight_decay=l2)\n",
    "    elif name == 'adam':\n",
    "        return torch.optim.Adam(parameters, weight_decay=l2) # use default lr\n",
    "    elif name == 'adamax':\n",
    "        return torch.optim.Adamax(parameters, weight_decay=l2) # use default lr\n",
    "    elif name == 'adadelta':\n",
    "        return torch.optim.Adadelta(parameters, lr=lr, weight_decay=l2)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported optimizer: {}\".format(name))\n",
    "\n",
    "\n",
    "def visualize_performance(performance, repo_name, show=False):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # accuracy\n",
    "    ax = plt.subplot(3, 2, 1)\n",
    "    ax.set_title('Accuracy')\n",
    "    ax.plot(performance['acc_test'], label='test')\n",
    "    ax.plot(performance['acc_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([0.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # precision\n",
    "    ax = plt.subplot(3, 2, 2)\n",
    "    ax.set_title('Precision')\n",
    "    ax.plot(performance['prec_test'], label='test')\n",
    "    ax.plot(performance['prec_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # recall\n",
    "    ax = plt.subplot(3, 2, 3)\n",
    "    ax.set_title('Recall')\n",
    "    ax.plot(performance['recall_test'], label='test')\n",
    "    ax.plot(performance['recall_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # f1\n",
    "    ax = plt.subplot(3, 2, 4)\n",
    "    ax.set_title('F1')\n",
    "    ax.plot(performance['f1_test'], label='test')\n",
    "    ax.plot(performance['f1_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # loss\n",
    "    ax = plt.subplot(3, 1, 3)\n",
    "    ax.set_title('loss')\n",
    "    ax.plot(performance['loss'])\n",
    "\n",
    "    ax.set_ylim([0, 0.5])\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.35,\n",
    "                        wspace=0.35)\n",
    "\n",
    "    if not show:\n",
    "        plt.savefig(repo_name + 'performance_vis.png')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class PythonLiteralOption(click.Option):\n",
    "    def type_cast_value(self, ctx, value):\n",
    "        # Either we denote a range, or a list with precise samples:\n",
    "        # 1) Range:\n",
    "        if 'range' in value:\n",
    "            idx_open = value.find('(')\n",
    "            idx_close = value.find(')')\n",
    "            # Get the range input\n",
    "            range_input = value[idx_open + 1:idx_close].split(',')\n",
    "            # Make it to numbers:\n",
    "            range_input = [int(num) for num in range_input]\n",
    "            try :\n",
    "                return list(range(*range_input))\n",
    "            except:\n",
    "                raise click.BadParameter(value)\n",
    "        else:\n",
    "            try:\n",
    "                return ast.literal_eval(value)\n",
    "            except:\n",
    "                raise click.BadParameter(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# For general use\n",
    "class GCNsimple(nn.Module):\n",
    "    \"\"\" It's a simple version of a GCN module operated on dependency graphs.\"\"\"\n",
    "    def __init__(self,  in_dim, out_dim, use_cuda = False, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "        self.W = nn.Linear(self.in_dim, self.out_dim, bias=bias)\n",
    "\n",
    "        # self.weight = self.W.weight\n",
    "        # self.bias = self.W.bias\n",
    "\n",
    "    def forward(self, x, lamb, relu=True):\n",
    "        # Push the data through gcn.\n",
    "        Lx = lamb.bmm(x)\n",
    "        LxW = self.W(Lx)\n",
    "        if relu:\n",
    "            gLxW = F.relu(LxW)\n",
    "        else:\n",
    "            gLxW = LxW\n",
    "\n",
    "        return gLxW\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\" A module that represents the multi-layer GCN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_layer, input_dimension=1, hidden_dimension=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_of_layer = num_of_layer\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.input_dimension = input_dimension\n",
    "        self.all_layer = nn.ModuleList()\n",
    "\n",
    "        if num_of_layer >0:\n",
    "            # first layer.\n",
    "            self.all_layer.append(GCNsimple(input_dimension,\n",
    "                                            hidden_dimension,\n",
    "                                            bias=bias))\n",
    "            for _ in range(num_of_layer - 1):\n",
    "                self.all_layer.append(GCNsimple(hidden_dimension, hidden_dimension))\n",
    "\n",
    "    def forward(self, x, lamb):\n",
    "        for i in range(self.num_of_layer):\n",
    "            x = self.all_layer[i](x, lamb)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.all_layer.parameters()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.all_layer.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from VGAE https://github.com/DaehanKim/vgae_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\t\tself.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X,adj):\n",
    "\t\thidden = self.base_gcn(X,adj)\n",
    "\t\tself.mean = self.gcn_mean(hidden,adj)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden,adj)\n",
    "\t\tgaussian_noise = torch.randn(X.size(0), hidden2_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "\n",
    "\tdef forward(self, X,adj):\n",
    "\t\tZ = self.encode(X,adj)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\tprint(np.shape(Z))\n",
    "\t\treturn A_pred\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs, adj):\n",
    "\t\tx = inputs\n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(adj, x)\n",
    "\t\toutputs = self.activation(x)\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "\tA_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "\treturn A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)\n",
    "\n",
    "\n",
    "class GAE(nn.Module):\n",
    "\tdef __init__(self,input_dim, hidden1_dim,hidden2_dim):\n",
    "\t\tsuper(GAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tz = self.mean = self.gcn_mean(hidden)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 450 \n",
    "hidden1_dim = 32\n",
    "hidden2_dim = 16\n",
    "use_feature = True\n",
    "num_epoch = 200\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def unpack_batch(batch, cuda):\n",
    "    if cuda:\n",
    "        inputs = [Variable(b.cuda()) for b in batch[:10]]\n",
    "        labels = Variable(batch[10].cuda())\n",
    "    else:\n",
    "        inputs = [Variable(b) for b in batch[:10]]\n",
    "        labels = Variable(batch[10])\n",
    "\n",
    "    # To have the possibility to pass custom adjacency matrix and words vectors to the prediction.\n",
    "    inputs += [None, None]  # It will be referenced as [cust_adj, cust_words]\n",
    "    tokens = batch[0]\n",
    "    head = batch[5]\n",
    "    subj_pos = batch[6]\n",
    "    obj_pos = batch[7]\n",
    "    lens = batch[1].eq(0).long().sum(1).squeeze()\n",
    "\n",
    "    return inputs, labels, tokens, head, subj_pos, obj_pos, lens\n",
    "\n",
    "\n",
    "# A helping function.\n",
    "def make_eyes(lambs, lens):\n",
    "    eyes = torch.zeros(lambs.shape)\n",
    "    for i, le in enumerate(lens):\n",
    "        eyes[i,:le, :le] = torch.eye(le)\n",
    "    return eyes\n",
    "\n",
    "\n",
    "class SentimTrainer:\n",
    "    def __init__(self, cfg, cuda=False):\n",
    "        # First check if we load the model:\n",
    "        if cfg['load_model']:\n",
    "            assert 'repo_name' in cfg, 'We need a file name to load the model.'\n",
    "            # Get all hyperparameter:\n",
    "            cfg.update(json.load(open(cfg['repo_name'] + 'config.json', 'r')))\n",
    "            # Set cuda:\n",
    "            cfg['cuda'] = cuda\n",
    "            self.cuda = cuda\n",
    "            checkpoint = torch.load(cfg['repo_name'] + 'model_params.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "            # just get the vocab dimensions:\n",
    "            cfg['vocab_len'] = checkpoint['embedding']['weight'].shape[0]\n",
    "            cfg['pos_vocab_len'] = checkpoint['pos_embedding']['weight'].shape[0]\n",
    "            cfg['lemma_vocab_len'] = checkpoint['lemma_embedding']['weight'].shape[0]\n",
    "            # don't need the checkpoints anymmore\n",
    "            del checkpoint\n",
    "\n",
    "            # Vocab initialisation:\n",
    "            self.vocab = Vocab()\n",
    "            self.pos_vocab = Vocab()\n",
    "            self.lemma_vocab = Vocab()\n",
    "\n",
    "            # make random model initialization:\n",
    "            self.random_model_init_(cfg)\n",
    "\n",
    "            # load pretrainded weights:\n",
    "            self.load(cfg['repo_name'], cuda=cfg['cuda'])\n",
    "\n",
    "        else:\n",
    "            # Set cuda:\n",
    "            cfg['cuda'] = cuda\n",
    "            self.cuda = cuda\n",
    "\n",
    "            # We need the vocabs given in cfg:\n",
    "            self.vocab = cfg['vocab']\n",
    "            self.pos_vocab = cfg['pos_vocab']\n",
    "            self.lemma_vocab = cfg['lemma_vocab']\n",
    "\n",
    "            # set up necessary vocab lens:\n",
    "            cfg['vocab_len'] = len(self.vocab)\n",
    "            cfg['pos_vocab_len'] = len(self.pos_vocab)\n",
    "            cfg['lemma_vocab_len'] = len(self.lemma_vocab)\n",
    "\n",
    "            # init parameter:\n",
    "            self.random_model_init_(cfg)\n",
    "\n",
    "            if self.vocab.word_embed is None: self.vocab.init_word_embed(cfg)\n",
    "            self.embedding = self.vocab.word_embed\n",
    "\n",
    "\n",
    "        self.model_type = cfg['model_type']\n",
    "        self.new_model = True\n",
    "\n",
    "        if self.cuda:\n",
    "            self.mp_model.cuda()\n",
    "            #self.output_layer.cuda()\n",
    "            self.embedding.cuda()\n",
    "            #self.first_layer.cuda()\n",
    "            #self.last_layer.cuda()\n",
    "            self.pos_embedding.cuda()\n",
    "            self.word_embedding.cuda()\n",
    "            self.lemma_embedding.cuda()\n",
    "\n",
    "        # Init optimizer:\n",
    "        #params = list(self.model.parameters()) \\\n",
    "        #         + list(self.pos_embedding.parameters()) \\\n",
    "        #         + list(self.word_embedding.parameters()) \\\n",
    "        #         + list(self.lemma_embedding.parameters())\n",
    "                # + list(self.first_layer.parameters()) \\\n",
    "                # + list(self.last_layer.parameters()) \\\n",
    "                # + list(self.output_layer.parameters()) \\\n",
    "\n",
    "        #self.optim = get_optimizer(cfg['optimizer'],\n",
    "        #                                         params,\n",
    "        #                                         cfg['lr'])\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def random_model_init_(self, cfg):\n",
    "        assert all(x in cfg for x in ['vocab_len', 'pos_vocab_len',\n",
    "                                      'lemma_vocab_len']), 'Please indicate the dimensions of the vocabularies.'\n",
    "        \n",
    "        # Embedding layer:\n",
    "        self.embedding = nn.Embedding(cfg['vocab_len'], cfg['input_dimension'])\n",
    "\n",
    "        # Trainable word embedding\n",
    "        self.word_embedding = nn.Embedding(cfg['vocab_len'], cfg['word_emb_dim'])\n",
    "        with torch.no_grad(): self.word_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.word_embedding.weight = nn.parameter.Parameter(self.word_embedding.weight, requires_grad=True) \n",
    "        # Make it to leaf variable again\n",
    "\n",
    "        # Pos embedding layer\n",
    "        self.pos_embedding = nn.Embedding(cfg['pos_vocab_len'], cfg['pos_emb_dim'])\n",
    "        with torch.no_grad(): self.pos_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.pos_embedding.weight = nn.parameter.Parameter(self.pos_embedding.weight, requires_grad=True)\n",
    "\n",
    "        # lemma embedding laayer\n",
    "        self.lemma_embedding = nn.Embedding(cfg['lemma_vocab_len'], cfg['lemma_emb_dim'])\n",
    "        with torch.no_grad(): self.lemma_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.lemma_embedding.weight = nn.parameter.Parameter(self.lemma_embedding.weight, requires_grad=True)\n",
    "        \n",
    "        # Add a first layer if wanted:\n",
    "        \n",
    "        #self.first_layer = GCNModel(num_of_layer=1,\n",
    "        #                    input_dimension=cfg['input_dimension'] + cfg['pos_emb_dim'] + cfg[\n",
    "         #                       'word_emb_dim'] + cfg['lemma_emb_dim'],\n",
    "         #                   hidden_dimension=cfg['hidden_dimension'],\n",
    "         #                   bias=cfg['bias'])\n",
    "        print(cfg['input_dimension'] + cfg['pos_emb_dim'] + \n",
    "                                 cfg['word_emb_dim'] + cfg['lemma_emb_dim'])\n",
    "        self.model = VGAE()\n",
    "        self.optim = Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Set up the model:\n",
    "        #self.mp_model = GCNModel(num_of_layer=cfg['num_of_layer'],\n",
    "        #                         input_dimension=cfg['hidden_dimension'],\n",
    "        #                         hidden_dimension=cfg['hidden_dimension'],\n",
    "        #                         bias=cfg['bias']\n",
    "         #                        )\n",
    "\n",
    "        # Last layer\n",
    "        #self.last_layer = GCNModel(num_of_layer=1,\n",
    "        #                           input_dimension=cfg['hidden_dimension'],\n",
    "         #                          hidden_dimension=cfg['hidden_dimension'],\n",
    "          #                         bias=cfg['bias'])\n",
    "        # Output layer:\n",
    "\n",
    "        #self.output_layer = nn.Linear(cfg['hidden_dimension'],\n",
    "        #                              cfg['num_of_classes'],\n",
    "        #                              bias=cfg['bias'])\n",
    "\n",
    "    def update(self, batch):\n",
    "        # Free the optimizer:\n",
    "        self.optim.zero_grad()\n",
    "        # Unwrap batch\n",
    "        \n",
    "        lambs, poss, texts, labels, lens, _, lemmas = batch[:7]\n",
    "        lambs = lambs.to(torch.int64)\n",
    "        # Adjacency in the first matrix are identity matrices:\n",
    "        eyes = make_eyes(lambs, lens)\n",
    "\n",
    "        # Set on cuda:\n",
    "        if self.cuda:\n",
    "            lambs = lambs.cuda()\n",
    "            texts = texts.cuda()\n",
    "            labels = labels.cuda()\n",
    "            eyes = eyes.cuda()\n",
    "            poss = poss.cuda()\n",
    "            lemmas = lemmas.cuda()\n",
    "\n",
    "        # Propagate through the the model\n",
    "        # Embedding layers\n",
    "        const_word_vec = self.embedding(texts)\n",
    "        word_vec = self.word_embedding(texts)\n",
    "        pos_vec = self.pos_embedding(poss)\n",
    "\n",
    "        lemma_vec = self.lemma_embedding(lemmas)\n",
    "\n",
    "        x = torch.cat([word_vec, pos_vec, const_word_vec, lemma_vec], dim=2)\n",
    "        x = x[0]\n",
    "        lambs = lambs[0].float()\n",
    "        \n",
    "        #print(np.shape(x))\n",
    "        #print(np.shape(lambs))\n",
    "        \n",
    "        \n",
    "        \n",
    "        A_pred = self.model(x,lambs)\n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        #pos_weight = float(lambs.shape[0] * lambs.shape[0] - lambs.sum()) / lambs.sum()\n",
    "        #weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "        #weight_tensor[weight_mask] = pos_weight\n",
    "        \n",
    "        const_word_vec_lab = self.embedding(labels)\n",
    "        word_vec_lab = self.word_embedding(labels)\n",
    "        pos_vec_lab = self.pos_embedding(poss)\n",
    "\n",
    "        lemma_ve_lab = self.lemma_embedding(lemmas)\n",
    "        labels = torch.cat([word_vec_lab, pos_vec_lab, const_word_vec_lab, lemma_ve_lab], dim=2)\n",
    "        print(np.shape(labels[0]))\n",
    "        print(np.shape(A_pred))\n",
    "        labels = labels[0]\n",
    "        norm = lambs.shape[0] * lambs.shape[0] / float((lambs.shape[0] * lambs.shape[0] - lambs.sum()) * 2)\n",
    "        \n",
    "        loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), \n",
    "                                                     labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # last layer:\n",
    "        #x = self.last_layer(x, eyes)\n",
    "\n",
    "        # Mean pooling: We expect xTs is of shape (batch_size, seq_len, hidden_dim)\n",
    "        #x = x.sum(1) / x.shape[1]\n",
    "\n",
    "        # Propagate through the last layer:\n",
    "        #x = self.output_layer(x)\n",
    "\n",
    "        # x.shape should be (batch_size, num_of_classes).\n",
    "        # label.shape should be (batch_size).\n",
    "        \n",
    "        #loss_val = self.loss(x, labels.to(torch.float32))\n",
    "\n",
    "        # Do the backward step:\n",
    "        #loss_val.backward()\n",
    "\n",
    "        # And the step \n",
    "        self.optim.step()\n",
    "\n",
    "        return loss_val.detach() / len(batch)\n",
    "\n",
    "    def predict(self, batch, debug=False, custom_vect_input=None):\n",
    "        lambs, poss, texts, labels, lens, _, lemmas = batch[:7]\n",
    "\n",
    "        # Adjacency in the first matrix are identity matrices:\n",
    "        eyes = make_eyes(lambs, lens)\n",
    "\n",
    "        # Set on cuda\n",
    "        if self.cuda:\n",
    "            lambs = lambs.cuda()\n",
    "            texts = texts.cuda()\n",
    "            labels = labels.cuda()\n",
    "            eyes = eyes.cuda()\n",
    "            poss = poss.cuda()\n",
    "            lemmas = lemmas.cuda()\n",
    "        if custom_vect_input is None:\n",
    "            # Embedding layers\n",
    "            const_word_vec = self.embedding(texts)\n",
    "            word_vec = self.word_embedding(texts)\n",
    "            pos_vec = self.pos_embedding(poss)\n",
    "            lemma_vec = self.lemma_embedding(lemmas)\n",
    "\n",
    "            x = torch.cat([word_vec, pos_vec, const_word_vec, lemma_vec], dim=2)\n",
    "        else:\n",
    "            x = custom_vect_input\n",
    "\n",
    "        # first layer:\n",
    "        # Adjacency matrix is the identity matrix.\n",
    "        x = self.first_layer(x, eyes)\n",
    "\n",
    "        # MPNN model\n",
    "        x = self.mp_model(x, lambs)\n",
    "\n",
    "        # last layer:\n",
    "        x = self.last_layer(x, eyes)\n",
    "\n",
    "        # Mean pooling: We expect xTs is of shape (batch_size, seq_len, hidden_dim)\n",
    "        x = x.sum(1) / x.shape[1]\n",
    "\n",
    "        # Propagate through the last layer:\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def load(self, filename, cuda=False):\n",
    "        if cuda:\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        try:\n",
    "            checkpoint = torch.load(filename + 'model_params.pt', map_location=device)\n",
    "        except BaseException:\n",
    "            print(\"Cannot load model from {}\".format(filename + 'model_params.pt'))\n",
    "            exit()\n",
    "\n",
    "        self.embedding.load_state_dict(checkpoint['embedding'])\n",
    "        self.pos_embedding.load_state_dict(checkpoint['pos_embedding'])\n",
    "        self.word_embedding.load_state_dict(checkpoint['word_embedding'])\n",
    "        self.lemma_embedding.load_state_dict(checkpoint['lemma_embedding'])\n",
    "        self.mp_model.load_state_dict(checkpoint['mp_model'])\n",
    "        #self.output_layer.load_state_dict(checkpoint['output_layer'])\n",
    "        #self.last_layer.load_state_dict(checkpoint['last_layer'])\n",
    "        #self.first_layer.load_state_dict(checkpoint['first_layer'])\n",
    "\n",
    "        self.vocab.load(filename + 'vocab.p')\n",
    "        self.pos_vocab.load(filename + 'pos_vocab.p')\n",
    "        self.lemma_vocab.load(filename + 'lemma_vocab.p')\n",
    "\n",
    "        self.vocab.word_embed = self.embedding\n",
    "        self.pos_vocab.word_embed = self.pos_embedding\n",
    "\n",
    "    def save(self, filename):\n",
    "        params = {\n",
    "            'embedding': self.embedding.state_dict(),\n",
    "            'pos_embedding': self.pos_embedding.state_dict(),\n",
    "            'mp_model': self.mp_model.state_dict(),\n",
    "            #'output_layer': self.output_layer.state_dict(),\n",
    "            #'last_layer': self.last_layer.state_dict(),\n",
    "            #'first_layer': self.first_layer.state_dict(),\n",
    "            'word_embedding': self.word_embedding.state_dict(),\n",
    "            'lemma_embedding': self.lemma_embedding.state_dict()\n",
    "        }\n",
    "        try:\n",
    "            torch.save(params, filename + 'model_params.pt')\n",
    "            self.vocab.save(filename + 'vocab.p')\n",
    "            self.pos_vocab.save(filename + 'pos_vocab.p')\n",
    "            self.lemma_vocab.save(filename + 'lemma_vocab.p')\n",
    "\n",
    "        except BaseException:\n",
    "            print(\"[Warning: Saving failed... continuing anyway.]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "mp_trainer = SentimTrainer(cfg, cuda=cfg['cuda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0:\n",
      "torch.Size([64, 16])\n",
      "torch.Size([64, 450])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([28800])) that is different to the input size (torch.Size([4096])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-060c60656420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-e48c9bd83d95>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlambs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), \n\u001b[0m\u001b[0;32m    224\u001b[0m                                                      labels.view(-1))\n\u001b[0;32m    225\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2904\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m             \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([28800])) that is different to the input size (torch.Size([4096])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "epochs = cfg['epochs']\n",
    "for epoch in tqdm(range(cfg['epochs'] + 1)):\n",
    "    if epoch > int(epochs*0.8):\n",
    "        for g in mp_trainer.optim.param_groups:\n",
    "            g['lr'] = g['lr']/10\n",
    "\n",
    "#     f1_macro_test, prec_macro_test, recall_macro_test, accuracy_test = test_performance(test_loader,\n",
    "#                                                                                         mp_trainer,\n",
    "#                                                                                         num_of_classes = cfg['output_dimesion'])\n",
    "#     f1_macro_train, prec_macro_train, recall_macro_train, accuracy_train = test_performance(train_loader,\n",
    "#                                                                                             mp_trainer,\n",
    "#                                                                                             num_of_classes = cfg['output_dimesion'])\n",
    "#     # Save vals for test:\n",
    "#     performance['acc_test'].append(accuracy_test)\n",
    "#     performance['prec_test'].append(prec_macro_test)\n",
    "#     performance['f1_test'].append(f1_macro_test)\n",
    "#     performance['recall_test'].append(recall_macro_test)\n",
    "\n",
    "#     # Save vals for train:\n",
    "#     performance['acc_train'].append(accuracy_train)\n",
    "#     performance['prec_train'].append(prec_macro_train)\n",
    "#     performance['f1_train'].append(f1_macro_train)\n",
    "#     performance['recall_train'].append(recall_macro_train)\n",
    "\n",
    "#     with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#         logfile.write('On test we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_test,\n",
    "#                                                                                                 f1_macro_test,\n",
    "#                                                                                                 prec_macro_test,\n",
    "#                                                                                                 recall_macro_test))\n",
    "#         logfile.write('On train we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_train,\n",
    "#                                                                                                 f1_macro_train,\n",
    "#                                                                                                 prec_macro_train,\n",
    "#                                                                                                 recall_macro_train))\n",
    "\n",
    "#     if accuracy_test > best_accuracy:\n",
    "#         mp_trainer.save(cfg['trainerfilename'])\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write(\n",
    "#                 'New best model with accuracy = {} saved at {}\\n'.format(accuracy_test, cfg['trainerfilename']))\n",
    "#         best_accuracy = accuracy_test\n",
    "#     else:\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write('Best model has accuracy = {}\\n'.format(best_accuracy))\n",
    "\n",
    "#     # save current performance.\n",
    "#     json.dump(performance, open(repo_name + 'performance.json', 'w'))\n",
    "#     # plot performance\n",
    "#     visualize_performance(performance, repo_name)\n",
    "\n",
    "    if epoch == range(cfg['epochs']):\n",
    "        # It's the last epoch, don't train again.\n",
    "        break\n",
    "\n",
    "    print('Train epoch {}:'.format(epoch))\n",
    "    epoch_loss = []\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        loss = mp_trainer.update(batch)\n",
    "        epoch_loss.append(float(loss.detach()))\n",
    "        print(loss)\n",
    "        if i % 50 == 0:\n",
    "            with open(cfg['logfilename'], 'a+') as logfile:\n",
    "                logfile.write('sample {} of {} in epoch {} of {}.\\n'.format(i,\n",
    "                                                                            len(train_loader),\n",
    "                                                                            epoch,\n",
    "                                                                            cfg['epochs']))\n",
    "\n",
    "    #performance['loss'].append(np.mean(np.array(epoch_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, filename='', load=False, threshold=0):\n",
    "        if load:\n",
    "            assert os.path.exists(filename), \"Vocab file does not exist at \" + filename\n",
    "\n",
    "            self.id2word, self.word2id = self.load(filename)\n",
    "            self.size = len(self.id2word)\n",
    "            self.threshold = threshold\n",
    "            self.wordCounter = None\n",
    "        else:\n",
    "            self.id2word, self.word2id = {}, {}\n",
    "            self.size = 0\n",
    "            self.threshold = threshold\n",
    "            # We always add some custom tokens into the vocabulary.\n",
    "            self.add_words(\n",
    "                {'<PAD>': float('inf'), '<UNK>': float('inf')})\n",
    "        self.word_embed = None\n",
    "\n",
    "    def add_words(self, counterOfTokens):\n",
    "        for item, value in counterOfTokens.items():\n",
    "            if value >= self.threshold:\n",
    "                if item not in self.word2id:\n",
    "                    # add it to the vocab\n",
    "                    self.word2id[item] = self.size\n",
    "                    self.id2word[self.size] = item\n",
    "                    self.size += 1\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as infile:\n",
    "            id2word = pickle.load(infile)\n",
    "            word2id = {word:id for id, word in id2word.items()}\n",
    "            self.id2word, self.word2id = id2word, word2id\n",
    "            self.size = len(self.id2word)\n",
    "\n",
    "        return id2word, word2id\n",
    "\n",
    "    def save(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "           \n",
    "        with open(filename, 'wb') as outfile:\n",
    "            pickle.dump(self.id2word, outfile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def init_word_embed(self, cfg, cache_dir='datasets/.word_vectors_cache'):\n",
    "        if cfg['word_vectors'] == 'Word2Vec':\n",
    "            from torchnlp.word_to_vector import FastText\n",
    "            all_word_vector = FastText(language=cfg['language'], cache=cache_dir, aligned=True)\n",
    "        else:\n",
    "            raise NotImplementedError('No word_vectors found which are called {}.'.format(cfg['word_vectors']))\n",
    "\n",
    "        # The the vectors only correspond to lower character words:\n",
    "        all_words = [word.lower() for word in list(self.word2id.keys())]\n",
    "        weights = all_word_vector[all_words]\n",
    "        \n",
    "        word_embed = torch.nn.Embedding(*weights.shape, _weight=weights)\n",
    "        #if cfg['device'] == 'cuda':\n",
    "        #    word_embed.cuda()\n",
    "\n",
    "        self.word_embed = word_embed\n",
    "        self.embed_size = weights.shape[1]\n",
    "\n",
    "    def words2vecs(self, words: list):\n",
    "        if not self.word_embed:\n",
    "            raise AttributeError(\"The word embeddings aren't initialized yet.\")\n",
    "        else:\n",
    "            vecs = self.word_embed(torch.tensor(self.map(words), requires_grad=False))\n",
    "        return vecs\n",
    "\n",
    "    def one_hot_ids2vecs(self, ids):\n",
    "        vecs = self.word_embed(ids)\n",
    "        return vecs\n",
    "\n",
    "    def map(self, token_list):\n",
    "        \"\"\"\n",
    "        Map a list of tokens to their ids.\n",
    "        \"\"\"\n",
    "        return [self.word2id[w] if w in self.word2id else self.word2id['<UNK>'] for w in token_list]\n",
    "\n",
    "    def unmap(self, idx_list):\n",
    "        \"\"\"\n",
    "        Unmap ids back to tokens.\n",
    "        \"\"\"\n",
    "        return [self.id2word[idx] for idx in idx_list]\n",
    "    \n",
    "def get_pos_vocab():\n",
    "    \"\"\"\n",
    "    Function to set up a part of speech vocabulary handcrafed.\n",
    "    \"\"\"\n",
    "    pos_id2word = {0: '<PAD>', 1: '<UNK>', 2: 'DET', 3: 'PROPN', 4: 'VERB', 5: 'PART', 6: 'ADJ', 7: 'PUNCT', 8: 'CCONJ',\n",
    "                   9: 'ADP', 10: 'PRON', 11: 'NOUN', 12: 'ADV', 13: 'INTJ', 14: 'NUM', 15: 'X', 16: 'SYM'}\n",
    "    pos_word2id = {word: id for id, word in pos_id2word.items()}\n",
    "    pos_vocab = Vocab()\n",
    "    pos_vocab.id2word = pos_id2word\n",
    "    pos_vocab.word2id = pos_word2id\n",
    "    pos_vocab.size = len(pos_vocab.id2word)\n",
    "    \n",
    "    return pos_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load sst data\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SSTData(Dataset):\n",
    "    def __init__(self,\n",
    "                 sst_data,\n",
    "                 vocab,\n",
    "                 nlp,\n",
    "                 lemma_vocab,\n",
    "                 pos_vocab = None,\n",
    "                 self_loop=True):\n",
    "\n",
    "        self.lemma_vocab = lemma_vocab\n",
    "        self.self_loop = self_loop\n",
    "        \n",
    "        \n",
    "        sst_data = [sample for sample in sst_data if sample['label'] != 'neutral']\n",
    "        self.sst_data = sst_data\n",
    "\n",
    "        self.sentiment_vocab = {'negative': 0, 'positive': 1}\n",
    "        \n",
    "        # Add sentencizer in the nlp if not already in it:\n",
    "        if \"sentencizer\" not in nlp.pipe_names:\n",
    "            # sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "            nlp.add_pipe('sentencizer', first=True)\n",
    "        self.nlp = nlp\n",
    "\n",
    "        self.vocab = vocab\n",
    "        if pos_vocab is None:\n",
    "            self.pos_vocab = get_pos_vocab()\n",
    "        else:\n",
    "            self.pos_vocab = pos_vocab\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.sst_data[idx]['text']\n",
    "        doc = self.nlp(txt)\n",
    "        if len(doc) <64:\n",
    "            txt = txt + ' PAD'*(64-len(doc))\n",
    "            doc = self.nlp(txt)\n",
    "        elif len(doc) > 64:\n",
    "            doc = doc[:64]\n",
    "\n",
    "        # make lambda\n",
    "\n",
    "        adj, root_id = doc_to_adj(doc, directed=False, self_loop=self.self_loop)\n",
    "\n",
    "        lamb = adj\n",
    "\n",
    "        # normalize\n",
    "        denom = lamb.sum(1)\n",
    "        lamb /= denom\n",
    "\n",
    "        # make text indices\n",
    "        token_ids = self.vocab.map([token.text for token in doc])\n",
    "        # create random array of floats in equal dimension to input_ids\n",
    "        rand = torch.rand(np.shape(vocab.map([token.text for token in doc])))\n",
    "        # where the random array is less than 0.15, we set true\n",
    "        mask_arr = rand < 0.15\n",
    "        # create selection from mask_arr\n",
    "        selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "        for i in selection: \n",
    "            token_ids[i] = 103\n",
    "        \n",
    "        # make pos ids\n",
    "        pos_ids = self.pos_vocab.map([token.pos_ for token in doc])\n",
    "\n",
    "        # make lemma ids\n",
    "        lemma_ids = self.lemma_vocab.map([token.lemma_ for token in doc])\n",
    "\n",
    "        # make label\n",
    "        label = self.vocab.map([token.text for token in doc])\n",
    "\n",
    "        return token_ids, pos_ids, lamb, label, root_id, lemma_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sst_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn_sentim(batch):\n",
    "    lens = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        lens.append(lamb.shape[1])\n",
    "\n",
    "    max_len = max(lens)\n",
    "    lambs = []\n",
    "    texts = []\n",
    "    poss = []\n",
    "    lemmas = []\n",
    "    labels = []\n",
    "    root_ids = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        # Big lamb\n",
    "        lamb_ = torch.zeros(1, max_len, max_len)\n",
    "        lamb_[0, :lamb.shape[0], :lamb.shape[1]] = lamb\n",
    "        lambs.append(lamb_)\n",
    "\n",
    "        # Big text\n",
    "        text_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        text_[0, :len(text)] = torch.tensor(text)\n",
    "        texts.append(text_)\n",
    "\n",
    "        # Big pos\n",
    "        pos_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        pos_[0, :len(pos)] = torch.tensor(pos)\n",
    "        poss.append(pos_)\n",
    "\n",
    "        # Big lemma\n",
    "        lemma_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        lemma_[0, :len(lemma)] = torch.tensor(lemma)\n",
    "        lemmas.append(lemma_)\n",
    "\n",
    "        # Big label:\n",
    "        label_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        label_[0, :len(label)] = torch.tensor(label)\n",
    "        labels.append(label_)\n",
    "\n",
    "        # Big root_id:\n",
    "        root_id_ = torch.ones(1) * root_id\n",
    "        root_ids.append(root_id_.long())\n",
    "\n",
    "    lambs = torch.cat(lambs, dim=0)\n",
    "    texts = torch.cat(texts, dim=0)\n",
    "    poss = torch.cat(poss, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    root_ids = torch.cat(root_ids, dim=0)\n",
    "    lemmas = torch.cat(lemmas, dim=0)\n",
    "\n",
    "    return lambs, poss, texts, labels, lens, root_ids, lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def doc_to_adj(sent, directed=True, self_loop=False):\n",
    "    # Sent should be a spacy document. Can also be longer than a sentence.\n",
    "    sent_len = len(sent)\n",
    "\n",
    "    ret = torch.zeros(sent_len, sent_len, dtype=torch.float32)\n",
    "\n",
    "    for token in sent:\n",
    "        for child in token.children:\n",
    "            if child.i >= sent_len:\n",
    "                print('Something goes wrong here.')\n",
    "                print(child.i, sent_len, sent, token.i, token)\n",
    "            ret[token.i, child.i] = 1\n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_id = token.i\n",
    "\n",
    "    if not directed:\n",
    "        ret = ret + ret.transpose(0, 1)\n",
    "\n",
    "    if self_loop:\n",
    "        for i in range(sent_len):\n",
    "            ret[i, i] = 1\n",
    "\n",
    "    return ret, root_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchnlp.datasets import smt_dataset\n",
    "import click\n",
    "\n",
    "import spacy\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"repo_name\": \"saved_models/sst_model1+/\", \n",
    "       \"epochs\": 50, \"optimizer\": \"adam\", \n",
    "       \"cuda\": False, \n",
    "       \"lr\": 0.0002, \n",
    "       \"num_of_layer\": 3, \n",
    "       \"hidden_dimension\": 10, \n",
    "       \"batch_size\": 1,\n",
    "       \"word_vectors\": \"Word2Vec\",\n",
    "       \"bias\": False, \n",
    "       \"pos_emb_dim\": 30,\n",
    "       \"model_type\": \"gcn\",\n",
    "       \"data_amount\": 1.0, \n",
    "       \"data_set\": \"sst\", \n",
    "       \"word_emb_dim\": 70,\n",
    "       \"lemma_emb_dim\": 50, \n",
    "       \"trainerfilename\": \"saved_models/sst_model1+/\",\n",
    "       \"logfilename\": \"saved_models/sst_model1+/log.txt\",\n",
    "       \"num_of_classes\": 64,\n",
    "       \"language\": \"en\",\n",
    "       \"normalize_lamb\": True,\n",
    "       \"laplacian\": False,\n",
    "       \"input_dimension\": 300,\n",
    "       'load_model' : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Prepare data ...\n"
     ]
    }
   ],
   "source": [
    "data_amount = 1\n",
    "\n",
    "print('Loading data...')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "cw = Counter()\n",
    "cl = Counter()\n",
    "\n",
    "\n",
    "\n",
    "train_set, test_set = smt_dataset(directory='datasets/sst_torchnlp', train=True, test=True, fine_grained=False)\n",
    "\n",
    "# only use the the percentage of data we want:\n",
    "train_set = train_set[:int(len(train_set) * data_amount)]\n",
    "test_set = test_set[:int(len(test_set) * data_amount)]\n",
    "\n",
    "# Count words:\n",
    "for sample in train_set + test_set: cw += Counter([token.text for token in nlp(sample['text'])])\n",
    "# Count lemma:\n",
    "for sample in train_set + test_set: cl += Counter([token.lemma_ for token in nlp(sample['text'])])\n",
    "\n",
    "vocab = Vocab()\n",
    "lemma_vocab = Vocab()\n",
    "\n",
    "\n",
    "# prepare vocab\n",
    "vocab.add_words(cw)\n",
    "#cfg['input_dimension'] = 300\n",
    "\n",
    "lemma_vocab.add_words(cl)\n",
    "pos_vocab = get_pos_vocab()\n",
    "\n",
    "# Save the parameter:\n",
    "#with open(repo_name + 'config.json', 'w') as fp:\n",
    "#    json.dump(cfg.get_as_dict(), fp)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Prepare data ...')\n",
    "\n",
    "\n",
    "train_data = SSTData(\n",
    "    train_set,\n",
    "    vocab,\n",
    "    nlp,\n",
    "    lemma_vocab,\n",
    ")\n",
    "test_data = SSTData(\n",
    "    test_set,\n",
    "    vocab,\n",
    "    nlp,\n",
    "    lemma_vocab,\n",
    ")\n",
    "\n",
    "cfg['vocab'] = vocab\n",
    "cfg['pos_vocab'] = lemma_vocab\n",
    "cfg['lemma_vocab'] = lemma_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = collate_fn_sentim\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=1,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x194fcb78fa0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(data_loader, mp_trainer, num_of_classes = 64):\n",
    "    # Evaluation\n",
    "    f1_micro = 0\n",
    "    prec_micro = 0\n",
    "    recall_micro = 0\n",
    "    accuracy = 0.\n",
    "    for batch in data_loader:\n",
    "        lambs, poss, texts, labels, lens = batch[:5]\n",
    "        logits = mp_trainer.predict(batch)\n",
    "\n",
    "        predicted_classes = torch.argmax(logits, dim=1)\n",
    "        if num_of_classes == 2:\n",
    "            average = 'binary'\n",
    "        else:\n",
    "            average = 'micro'\n",
    "        f1_micro += f1_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        prec_micro += precision_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        recall_micro += recall_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        accuracy += accuracy_score(labels.cpu(), predicted_classes.cpu())\n",
    "\n",
    "    f1_micro /= len(data_loader)\n",
    "    prec_micro /= len(data_loader)\n",
    "    recall_micro /= len(data_loader)\n",
    "    accuracy /= len(data_loader)\n",
    "\n",
    "    return f1_micro, prec_micro, recall_micro, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'saved_models/sst_model1+/'\n",
    "cfg['trainerfilename'] = repo_name\n",
    "cfg['logfilename'] = repo_name + 'log.txt'\n",
    "\n",
    "# Task specific settings\n",
    "cfg['num_of_classes'] = 64\n",
    "cfg['language'] = 'en'\n",
    "\n",
    "# model specific settings\n",
    "cfg['normalize_lamb'] = True\n",
    "cfg['laplacian'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    }
   ],
   "source": [
    "with open(cfg['logfilename'], 'a+') as logfile:\n",
    "    logfile.write(\n",
    "        '{} training samples set into {} batches, {} test samples set into {} batches\\n'.format(len(train_data),\n",
    "                                                                                                len(train_loader),\n",
    "                                                                                                len(test_data),\n",
    "                                                                                                len(test_loader)))\n",
    "    logfile.write('Start training.\\n')\n",
    "\n",
    "all_losses = []\n",
    "performance = {'acc_test': [], 'prec_test': [], 'f1_test': [], 'recall_test': [],\n",
    "               'acc_train': [], 'prec_train': [], 'f1_train': [], 'recall_train': [],\n",
    "               'loss': []}\n",
    "print('Start training')\n",
    "best_accuracy = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "mp_trainer = SentimTrainer(cfg, cuda=cfg['cuda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-060c60656420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-0edb74e6b38d>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_word_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemma_vec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mA_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), \n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "epochs = cfg['epochs']\n",
    "for epoch in tqdm(range(cfg['epochs'] + 1)):\n",
    "    if epoch > int(epochs*0.8):\n",
    "        for g in mp_trainer.optim.param_groups:\n",
    "            g['lr'] = g['lr']/10\n",
    "\n",
    "#     f1_macro_test, prec_macro_test, recall_macro_test, accuracy_test = test_performance(test_loader,\n",
    "#                                                                                         mp_trainer,\n",
    "#                                                                                         num_of_classes = cfg['output_dimesion'])\n",
    "#     f1_macro_train, prec_macro_train, recall_macro_train, accuracy_train = test_performance(train_loader,\n",
    "#                                                                                             mp_trainer,\n",
    "#                                                                                             num_of_classes = cfg['output_dimesion'])\n",
    "#     # Save vals for test:\n",
    "#     performance['acc_test'].append(accuracy_test)\n",
    "#     performance['prec_test'].append(prec_macro_test)\n",
    "#     performance['f1_test'].append(f1_macro_test)\n",
    "#     performance['recall_test'].append(recall_macro_test)\n",
    "\n",
    "#     # Save vals for train:\n",
    "#     performance['acc_train'].append(accuracy_train)\n",
    "#     performance['prec_train'].append(prec_macro_train)\n",
    "#     performance['f1_train'].append(f1_macro_train)\n",
    "#     performance['recall_train'].append(recall_macro_train)\n",
    "\n",
    "#     with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#         logfile.write('On test we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_test,\n",
    "#                                                                                                 f1_macro_test,\n",
    "#                                                                                                 prec_macro_test,\n",
    "#                                                                                                 recall_macro_test))\n",
    "#         logfile.write('On train we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_train,\n",
    "#                                                                                                 f1_macro_train,\n",
    "#                                                                                                 prec_macro_train,\n",
    "#                                                                                                 recall_macro_train))\n",
    "\n",
    "#     if accuracy_test > best_accuracy:\n",
    "#         mp_trainer.save(cfg['trainerfilename'])\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write(\n",
    "#                 'New best model with accuracy = {} saved at {}\\n'.format(accuracy_test, cfg['trainerfilename']))\n",
    "#         best_accuracy = accuracy_test\n",
    "#     else:\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write('Best model has accuracy = {}\\n'.format(best_accuracy))\n",
    "\n",
    "#     # save current performance.\n",
    "#     json.dump(performance, open(repo_name + 'performance.json', 'w'))\n",
    "#     # plot performance\n",
    "#     visualize_performance(performance, repo_name)\n",
    "\n",
    "    if epoch == range(cfg['epochs']):\n",
    "        # It's the last epoch, don't train again.\n",
    "        break\n",
    "\n",
    "    print('Train epoch {}:'.format(epoch))\n",
    "    epoch_loss = []\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        loss = mp_trainer.update(batch)\n",
    "        epoch_loss.append(float(loss.detach()))\n",
    "        print(loss)\n",
    "        if i % 50 == 0:\n",
    "            with open(cfg['logfilename'], 'a+') as logfile:\n",
    "                logfile.write('sample {} of {} in epoch {} of {}.\\n'.format(i,\n",
    "                                                                            len(train_loader),\n",
    "                                                                            epoch,\n",
    "                                                                            cfg['epochs']))\n",
    "\n",
    "    #performance['loss'].append(np.mean(np.array(epoch_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10,64,450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    #print(np.shape(batch))\n",
    "    print(np.shape(batch[0]))\n",
    "    #print(batch[0][3][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"election on an anti-slavery platform, an initial seven slave states declared their secession from the country to form the Confederacy. War broke out in April 1861 \"\n",
    "        \n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "election on an anti-slavery platform, an initial seven slave states declared their secession from the country to form the Confederacy. War broke out in April 1861 "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64-len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(doc) <64:\n",
    "    txt = txt + ' PAD'*(63-len(doc))\n",
    "    doc = nlp(txt)\n",
    "elif len(doc) > 64:\n",
    "    doc = doc[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9114,\n",
       " 114,\n",
       " 117,\n",
       " 2873,\n",
       " 30,\n",
       " 7398,\n",
       " 1,\n",
       " 28,\n",
       " 117,\n",
       " 7540,\n",
       " 6888,\n",
       " 7323,\n",
       " 16878,\n",
       " 301,\n",
       " 191,\n",
       " 1,\n",
       " 455,\n",
       " 8,\n",
       " 5943,\n",
       " 6,\n",
       " 3110,\n",
       " 8,\n",
       " 1,\n",
       " 37,\n",
       " 4693,\n",
       " 6097,\n",
       " 220,\n",
       " 125,\n",
       " 14842,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election',\n",
       " 'on',\n",
       " 'an',\n",
       " 'anti',\n",
       " '-',\n",
       " 'slavery',\n",
       " 'platform',\n",
       " ',',\n",
       " 'an',\n",
       " 'initial',\n",
       " 'seven',\n",
       " 'slave',\n",
       " 'states',\n",
       " 'declared',\n",
       " 'their',\n",
       " 'secession',\n",
       " 'from',\n",
       " 'the',\n",
       " 'country',\n",
       " 'to',\n",
       " 'form',\n",
       " 'the',\n",
       " 'Confederacy',\n",
       " '.',\n",
       " 'War',\n",
       " 'broke',\n",
       " 'out',\n",
       " 'in',\n",
       " 'April',\n",
       " '1861',\n",
       " ' ',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PART',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'NUM',\n",
       " 'SPACE',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PROPN']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election',\n",
       " 'on',\n",
       " 'an',\n",
       " 'anti',\n",
       " '-',\n",
       " 'slavery',\n",
       " 'platform',\n",
       " ',',\n",
       " 'an',\n",
       " 'initial',\n",
       " 'seven',\n",
       " 'slave',\n",
       " 'state',\n",
       " 'declare',\n",
       " 'their',\n",
       " 'secession',\n",
       " 'from',\n",
       " 'the',\n",
       " 'country',\n",
       " 'to',\n",
       " 'form',\n",
       " 'the',\n",
       " 'Confederacy',\n",
       " '.',\n",
       " 'war',\n",
       " 'break',\n",
       " 'out',\n",
       " 'in',\n",
       " 'April',\n",
       " '1861',\n",
       " ' ',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, root_id = doc_to_adj(doc, directed=False, self_loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats in equal dimension to input_ids\n",
    "rand = torch.rand(np.shape(vocab.map([token.text for token in doc])))\n",
    "# where the random array is less than 0.15, we set true\n",
    "mask_arr = rand < 0.15\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selection from mask_arr\n",
    "selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in selection: \n",
    "    inputs[i] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
