{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrapt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForMaskedLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     33\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     can_return_loss,\n\u001b[0;32m     42\u001b[0m     expand_dims,\n\u001b[0;32m     43\u001b[0m     find_labels,\n\u001b[0;32m     44\u001b[0m     flatten_dict,\n\u001b[0;32m     45\u001b[0m     is_jax_tensor,\n\u001b[0;32m     46\u001b[0m     is_numpy_array,\n\u001b[0;32m     47\u001b[0m     is_tensor,\n\u001b[0;32m     48\u001b[0m     is_tf_tensor,\n\u001b[0;32m     49\u001b[0m     is_torch_device,\n\u001b[0;32m     50\u001b[0m     is_torch_tensor,\n\u001b[0;32m     51\u001b[0m     reshape,\n\u001b[0;32m     52\u001b[0m     squeeze,\n\u001b[0;32m     53\u001b[0m     tensor_size,\n\u001b[0;32m     54\u001b[0m     to_numpy,\n\u001b[0;32m     55\u001b[0m     to_py_obj,\n\u001b[0;32m     56\u001b[0m     transpose,\n\u001b[0;32m     57\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     61\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     send_example_telemetry,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     90\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     91\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     torch_version,\n\u001b[0;32m    167\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:95\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:387\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wrapt'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#text = (\"After Abraham Lincoln won the November 1860 presidential \"\n",
    "#        \"election on an anti-slavery platform, an initial seven \"\n",
    "#        \"slave states declared their secession from the country \"\n",
    "#        \"to form the Confederacy. War broke out in April 1861 \"\n",
    "#        \"when secessionist forces attacked Fort Sumter in South \"\n",
    "#        \"Carolina, just over a month after Lincoln's \"\n",
    "#        \"inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats in equal dimension to input_ids\n",
    "#rand = torch.rand(inputs.input_ids.shape)\n",
    "# where the random array is less than 0.15, we set true\n",
    "#mask_arr = rand < 0.15\n",
    "#mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(inputs.input_ids != 101) * (inputs.input_ids != 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\n",
    "#mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selection from mask_arr\n",
    "#selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
    "#selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply selection index to inputs.input_ids, adding MASK tokens\n",
    "#inputs.input_ids[0, selection] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'artist' from 'matplotlib' (C:\\Users\\Basit\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mUtility functions for torch.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclick\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mast\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m### torch specific functions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_optimizer\u001b[39m(name, parameters, lr, l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\pyplot.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\colorbar.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, collections, cm, colors, contour, ticker\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\collections.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, cm, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, docstring,\n\u001b[0;32m     21\u001b[0m                hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\lines.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, artist, cbook, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, docstring, rcParams\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artist, allow_rasterization\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     _to_unmasked_float_array, ls_mapper, ls_mapper_r, STEP_LOOKUP_MAP)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'artist' from 'matplotlib' (C:\\Users\\Basit\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Utility functions for torch.\n",
    "\"\"\"\n",
    "\n",
    "import click, ast, torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### torch specific functions\n",
    "def get_optimizer(name, parameters, lr, l2=0):\n",
    "    if name == 'sgd':\n",
    "        return torch.optim.SGD(parameters, lr=lr, weight_decay=l2)\n",
    "    elif name == 'adam':\n",
    "        return torch.optim.Adam(parameters, weight_decay=l2) # use default lr\n",
    "    elif name == 'adamax':\n",
    "        return torch.optim.Adamax(parameters, weight_decay=l2) # use default lr\n",
    "    elif name == 'adadelta':\n",
    "        return torch.optim.Adadelta(parameters, lr=lr, weight_decay=l2)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported optimizer: {}\".format(name))\n",
    "\n",
    "\n",
    "def visualize_performance(performance, repo_name, show=False):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # accuracy\n",
    "    ax = plt.subplot(3, 2, 1)\n",
    "    ax.set_title('Accuracy')\n",
    "    ax.plot(performance['acc_test'], label='test')\n",
    "    ax.plot(performance['acc_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([0.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # precision\n",
    "    ax = plt.subplot(3, 2, 2)\n",
    "    ax.set_title('Precision')\n",
    "    ax.plot(performance['prec_test'], label='test')\n",
    "    ax.plot(performance['prec_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # recall\n",
    "    ax = plt.subplot(3, 2, 3)\n",
    "    ax.set_title('Recall')\n",
    "    ax.plot(performance['recall_test'], label='test')\n",
    "    ax.plot(performance['recall_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # f1\n",
    "    ax = plt.subplot(3, 2, 4)\n",
    "    ax.set_title('F1')\n",
    "    ax.plot(performance['f1_test'], label='test')\n",
    "    ax.plot(performance['f1_train'], label='train')\n",
    "\n",
    "    ax.set_ylim([.5, 1])\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # loss\n",
    "    ax = plt.subplot(3, 1, 3)\n",
    "    ax.set_title('loss')\n",
    "    ax.plot(performance['loss'])\n",
    "\n",
    "    ax.set_ylim([0, 0.5])\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.35,\n",
    "                        wspace=0.35)\n",
    "\n",
    "    if not show:\n",
    "        plt.savefig(repo_name + 'performance_vis.png')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class PythonLiteralOption(click.Option):\n",
    "    def type_cast_value(self, ctx, value):\n",
    "        # Either we denote a range, or a list with precise samples:\n",
    "        # 1) Range:\n",
    "        if 'range' in value:\n",
    "            idx_open = value.find('(')\n",
    "            idx_close = value.find(')')\n",
    "            # Get the range input\n",
    "            range_input = value[idx_open + 1:idx_close].split(',')\n",
    "            # Make it to numbers:\n",
    "            range_input = [int(num) for num in range_input]\n",
    "            try :\n",
    "                return list(range(*range_input))\n",
    "            except:\n",
    "                raise click.BadParameter(value)\n",
    "        else:\n",
    "            try:\n",
    "                return ast.literal_eval(value)\n",
    "            except:\n",
    "                raise click.BadParameter(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# For general use\n",
    "class GCNsimple(nn.Module):\n",
    "    \"\"\" It's a simple version of a GCN module operated on dependency graphs.\"\"\"\n",
    "    def __init__(self,  in_dim, out_dim, use_cuda = False, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "        self.W = nn.Linear(self.in_dim, self.out_dim, bias=bias)\n",
    "\n",
    "        # self.weight = self.W.weight\n",
    "        # self.bias = self.W.bias\n",
    "\n",
    "    def forward(self, x, lamb, relu=True):\n",
    "        # Push the data through gcn.\n",
    "        Lx = lamb.bmm(x)\n",
    "        LxW = self.W(Lx)\n",
    "        if relu:\n",
    "            gLxW = F.relu(LxW)\n",
    "        else:\n",
    "            gLxW = LxW\n",
    "\n",
    "        return gLxW\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\" A module that represents the multi-layer GCN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_layer, input_dimension=1, hidden_dimension=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_of_layer = num_of_layer\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.input_dimension = input_dimension\n",
    "        self.all_layer = nn.ModuleList()\n",
    "\n",
    "        if num_of_layer >0:\n",
    "            # first layer.\n",
    "            self.all_layer.append(GCNsimple(input_dimension,\n",
    "                                            hidden_dimension,\n",
    "                                            bias=bias))\n",
    "            for _ in range(num_of_layer - 1):\n",
    "                self.all_layer.append(GCNsimple(hidden_dimension, hidden_dimension))\n",
    "\n",
    "    def forward(self, x, lamb):\n",
    "        for i in range(self.num_of_layer):\n",
    "            x = self.all_layer[i](x, lamb)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.all_layer.parameters()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.all_layer.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from VGAE https://github.com/DaehanKim/vgae_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim, activation=F.relu)\n",
    "\t\tself.base2_gcn = GraphConvSparse(hidden1_dim, hidden2_dim, activation=F.relu)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden2_dim, hidden3_dim, activation=None)\n",
    "\t\tself.gcn_logstddev = GraphConvSparse(hidden2_dim, hidden3_dim, activation=None)\n",
    "\t\tself.dec_1 = GraphConvSparse(hidden3_dim, hidden2_dim, activation=F.relu)\n",
    "\t\tself.dec_2 = GraphConvSparse(hidden2_dim, hidden1_dim, activation=F.relu)\n",
    "\t\tself.dec_3 = GraphConvSparse(hidden1_dim, output_dim, activation=F.sigmoid)\n",
    "        \n",
    "        \n",
    "\tdef encode(self, X,adj):\n",
    "\t\thidden = self.base_gcn(X,adj)\n",
    "\t\thidden = self.base2_gcn(hidden,adj)\n",
    "\t\tself.mean = self.gcn_mean(hidden,adj)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden,adj)\n",
    "\t\tgaussian_noise = torch.randn(X.size(0), hidden3_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "    \n",
    "\tdef decode(self,Z,adj):\n",
    "\t\tA_pred = self.dec_1(Z,adj)\n",
    "\t\tA_pred = self.dec_2(A_pred,adj)\n",
    "\t\tA_pred = self.dec_3(A_pred,adj)\n",
    "\t\treturn A_pred\n",
    "\n",
    "\tdef forward(self, X,adj):\n",
    "\t\tZ = self.encode(X,adj)\n",
    "\t\tA_pred = self.decode(Z,adj)\n",
    "\t\treturn A_pred\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs, adj):\n",
    "\t\tx = inputs\n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(adj, x)\n",
    "\t\tif self.activation:\n",
    "\t\t\toutputs = self.activation(x)\n",
    "\t\t\treturn outputs\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)\n",
    "\n",
    "\n",
    "class GAE(nn.Module):\n",
    "\tdef __init__(self,input_dim, hidden1_dim,hidden2_dim):\n",
    "\t\tsuper(GAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tz = self.mean = self.gcn_mean(hidden)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 450\n",
    "hidden1_dim = 256\n",
    "hidden2_dim = 128\n",
    "hidden3_dim = 100\n",
    "output_dim = 450\n",
    "use_feature = True\n",
    "num_epoch = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def unpack_batch(batch, cuda):\n",
    "    if cuda:\n",
    "        inputs = [Variable(b.cuda()) for b in batch[:10]]\n",
    "        labels = Variable(batch[10].cuda())\n",
    "    else:\n",
    "        inputs = [Variable(b) for b in batch[:10]]\n",
    "        labels = Variable(batch[10])\n",
    "\n",
    "    # To have the possibility to pass custom adjacency matrix and words vectors to the prediction.\n",
    "    inputs += [None, None]  # It will be referenced as [cust_adj, cust_words]\n",
    "    tokens = batch[0]\n",
    "    head = batch[5]\n",
    "    subj_pos = batch[6]\n",
    "    obj_pos = batch[7]\n",
    "    lens = batch[1].eq(0).long().sum(1).squeeze()\n",
    "\n",
    "    return inputs, labels, tokens, head, subj_pos, obj_pos, lens\n",
    "\n",
    "\n",
    "# A helping function.\n",
    "def make_eyes(lambs, lens):\n",
    "    eyes = torch.zeros(lambs.shape)\n",
    "    for i, le in enumerate(lens):\n",
    "        eyes[i,:le, :le] = torch.eye(le)\n",
    "    return eyes\n",
    "\n",
    "\n",
    "class MLM:\n",
    "    def __init__(self, cfg, cuda=False):\n",
    "        # First check if we load the model:\n",
    "        if cfg['load_model']:\n",
    "            assert 'repo_name' in cfg, 'We need a file name to load the model.'\n",
    "            # Get all hyperparameter:\n",
    "            cfg.update(json.load(open(cfg['repo_name'] + 'config.json', 'r')))\n",
    "            # Set cuda:\n",
    "            cfg['cuda'] = cuda\n",
    "            self.cuda = cuda\n",
    "            checkpoint = torch.load(cfg['repo_name'] + 'model_params.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "            # just get the vocab dimensions:\n",
    "            cfg['vocab_len'] = checkpoint['embedding']['weight'].shape[0]\n",
    "            cfg['pos_vocab_len'] = checkpoint['pos_embedding']['weight'].shape[0]\n",
    "            cfg['lemma_vocab_len'] = checkpoint['lemma_embedding']['weight'].shape[0]\n",
    "            # don't need the checkpoints anymmore\n",
    "            del checkpoint\n",
    "\n",
    "            # Vocab initialisation:\n",
    "            self.vocab = Vocab()\n",
    "            self.pos_vocab = Vocab()\n",
    "            self.lemma_vocab = Vocab()\n",
    "\n",
    "            # make random model initialization:\n",
    "            self.random_model_init_(cfg)\n",
    "\n",
    "            # load pretrainded weights:\n",
    "            self.load(cfg['repo_name'], cuda=cfg['cuda'])\n",
    "\n",
    "        else:\n",
    "            # Set cuda:\n",
    "            cfg['cuda'] = cuda\n",
    "            self.cuda = cuda\n",
    "\n",
    "            # We need the vocabs given in cfg:\n",
    "            self.vocab = cfg['vocab']\n",
    "            self.pos_vocab = cfg['pos_vocab']\n",
    "            self.lemma_vocab = cfg['lemma_vocab']\n",
    "\n",
    "            # set up necessary vocab lens:\n",
    "            cfg['vocab_len'] = len(self.vocab)\n",
    "            cfg['pos_vocab_len'] = len(self.pos_vocab)\n",
    "            cfg['lemma_vocab_len'] = len(self.lemma_vocab)\n",
    "\n",
    "            # init parameter:\n",
    "            self.random_model_init_(cfg)\n",
    "\n",
    "            if self.vocab.word_embed is None: self.vocab.init_word_embed(cfg)\n",
    "            self.embedding = self.vocab.word_embed\n",
    "\n",
    "\n",
    "        self.model_type = cfg['model_type']\n",
    "        self.new_model = True\n",
    "\n",
    "        if self.cuda:\n",
    "            self.mp_model.cuda()\n",
    "            #self.output_layer.cuda()\n",
    "            self.embedding.cuda()\n",
    "            #self.first_layer.cuda()\n",
    "            #self.last_layer.cuda()\n",
    "            self.pos_embedding.cuda()\n",
    "            self.word_embedding.cuda()\n",
    "            self.lemma_embedding.cuda()\n",
    "\n",
    "        # Init optimizer:\n",
    "        #params = list(self.model.parameters()) \\\n",
    "        #         + list(self.pos_embedding.parameters()) \\\n",
    "        #         + list(self.word_embedding.parameters()) \\\n",
    "        #         + list(self.lemma_embedding.parameters())\n",
    "                # + list(self.first_layer.parameters()) \\\n",
    "                # + list(self.last_layer.parameters()) \\\n",
    "                # + list(self.output_layer.parameters()) \\\n",
    "\n",
    "        #self.optim = get_optimizer(cfg['optimizer'],\n",
    "        #                                         params,\n",
    "        #                                         cfg['lr'])\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def random_model_init_(self, cfg):\n",
    "        assert all(x in cfg for x in ['vocab_len', 'pos_vocab_len',\n",
    "                                      'lemma_vocab_len']), 'Please indicate the dimensions of the vocabularies.'\n",
    "        \n",
    "        # Embedding layer:\n",
    "        self.embedding = nn.Embedding(cfg['vocab_len'], cfg['input_dimension'])\n",
    "\n",
    "        # Trainable word embedding\n",
    "        self.word_embedding = nn.Embedding(cfg['vocab_len'], cfg['word_emb_dim'])\n",
    "        with torch.no_grad(): self.word_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.word_embedding.weight = nn.parameter.Parameter(self.word_embedding.weight, requires_grad=True) \n",
    "        # Make it to leaf variable again\n",
    "\n",
    "        # Pos embedding layer\n",
    "        self.pos_embedding = nn.Embedding(cfg['pos_vocab_len'], cfg['pos_emb_dim'])\n",
    "        with torch.no_grad(): self.pos_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.pos_embedding.weight = nn.parameter.Parameter(self.pos_embedding.weight, requires_grad=True)\n",
    "\n",
    "        # lemma embedding laayer\n",
    "        self.lemma_embedding = nn.Embedding(cfg['lemma_vocab_len'], cfg['lemma_emb_dim'])\n",
    "        with torch.no_grad(): self.lemma_embedding.weight[0] = 0. # set '<PAD>' to zero\n",
    "        self.lemma_embedding.weight = nn.parameter.Parameter(self.lemma_embedding.weight, requires_grad=True)\n",
    "        \n",
    "        # Add a first layer if wanted:\n",
    "        \n",
    "        #self.first_layer = GCNModel(num_of_layer=1,\n",
    "        #                    input_dimension=cfg['input_dimension'] + cfg['pos_emb_dim'] + cfg[\n",
    "         #                       'word_emb_dim'] + cfg['lemma_emb_dim'],\n",
    "         #                   hidden_dimension=cfg['hidden_dimension'],\n",
    "         #                   bias=cfg['bias'])\n",
    "        print(cfg['input_dimension'] + cfg['pos_emb_dim'] + \n",
    "                                 cfg['word_emb_dim'] + cfg['lemma_emb_dim'])\n",
    "        self.model = VGAE()\n",
    "        self.optim = Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Set up the model:\n",
    "        #self.mp_model = GCNModel(num_of_layer=cfg['num_of_layer'],\n",
    "        #                         input_dimension=cfg['hidden_dimension'],\n",
    "        #                         hidden_dimension=cfg['hidden_dimension'],\n",
    "        #                         bias=cfg['bias']\n",
    "         #                        )\n",
    "\n",
    "        # Last layer\n",
    "        #self.last_layer = GCNModel(num_of_layer=1,\n",
    "        #                           input_dimension=cfg['hidden_dimension'],\n",
    "         #                          hidden_dimension=cfg['hidden_dimension'],\n",
    "          #                         bias=cfg['bias'])\n",
    "        # Output layer:\n",
    "\n",
    "        #self.output_layer = nn.Linear(cfg['hidden_dimension'],\n",
    "        #                              cfg['num_of_classes'],\n",
    "        #                              bias=cfg['bias'])\n",
    "\n",
    "    def update(self, batch):\n",
    "        # Free the optimizer:\n",
    "        #self.optim.zero_grad()\n",
    "        # Unwrap batch\n",
    "        batch_loss = []\n",
    "        \n",
    "        \n",
    "        lambs, poss, texts, labels, lens, _, lemmas = batch[:7]\n",
    "        lambs = lambs.to(torch.int64)\n",
    "            # Adjacency in the first matrix are identity matrices:\n",
    "            #eyes = make_eyes(lambs, lens)\n",
    "\n",
    "            # Set on cuda:\n",
    "#             if self.cuda:\n",
    "#                 lambs = lambs.cuda()\n",
    "#                 texts = texts.cuda()\n",
    "#                 labels = labels.cuda()\n",
    "#                 eyes = eyes.cuda()\n",
    "#                 poss = poss.cuda()\n",
    "#                 lemmas = lemmas.cuda()\n",
    "        #print(np.shape(poss))\n",
    "            # Propagate through the the model\n",
    "            # Embedding layers\n",
    "        \n",
    "            \n",
    "        const_word_vec = self.embedding(texts)\n",
    "        word_vec = self.word_embedding(texts)\n",
    "        pos_vec = self.pos_embedding(poss)\n",
    "        lemma_vec = self.lemma_embedding(lemmas)\n",
    "\n",
    "\n",
    "\n",
    "        #print(np.shape(texts),np.shape(poss),np.shape(lemmas))\n",
    "        #x = torch.cat([texts, poss, lemmas], dim=0).transpose(0,1)\n",
    "\n",
    "       # print(texts[:5],poss[:5],lemmas[:5],x[:5])\n",
    "        x = torch.cat([word_vec, pos_vec, const_word_vec, lemma_vec], dim=2)\n",
    "        #print(np.shape(x))\n",
    "        #x = x[0]\n",
    "        x = x.float()\n",
    "        #print(x[0])\n",
    "        lambs = lambs.float()\n",
    "        #print(np.shape(lambs))\n",
    "\n",
    "        #print(np.shape(x))\n",
    "        #print(np.shape(lambs))\n",
    "        const_word_vec_lab = self.embedding(labels)\n",
    "        word_vec_lab = self.word_embedding(labels)\n",
    "        pos_vec_lab = self.pos_embedding(poss)\n",
    "\n",
    "        lemma_ve_lab = self.lemma_embedding(lemmas)\n",
    "        labels = torch.cat([word_vec_lab, pos_vec_lab, const_word_vec_lab, lemma_ve_lab], dim=2)\n",
    "\n",
    "        #labels = labels[0]\n",
    "        #print(labels)\n",
    "\n",
    "        norm = lambs.shape[0] * lambs.shape[0] / float((lambs.shape[0] * lambs.shape[0] - lambs.sum()) * 2)\n",
    "        \n",
    "        #print(np.shape(x),np.shape(lambs))\n",
    "        for sample in range(len(batch[0])):\n",
    "            #try:\n",
    "            A_pred = self.model(x[sample],lambs[sample])\n",
    "            #print(np.shape(A_pred))\n",
    "            #print(np.shape(labels))\n",
    "            self.optim.zero_grad()\n",
    "            #make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
    "\n",
    "            #pos_weight = float(lambs.shape[0] * lambs.shape[0] - lambs.sum()) / lambs.sum()\n",
    "            #weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "            #weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "\n",
    "            #for i in A_pred[0]:\n",
    "            #    print(i)\n",
    "            #print(labels)\n",
    "\n",
    "            loss = log_lik = norm*F.cross_entropy(A_pred,labels[sample])\n",
    "            #print('loss : ', log_lik)\n",
    "\n",
    "            #kl loss from gpt\n",
    "            kl_divergence = torch.sum(1 + self.model.logstd - self.model.mean.pow(2) - self.model.logstd.exp())\n",
    "\n",
    "            #loss from vgae paper\n",
    "            #kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*self.model.logstd - self.model.mean**2 - torch.exp(self.model.logstd)**2).sum(1).mean()\n",
    "            loss += kl_divergence\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "            batch_loss.append(float(loss.detach()))\n",
    "            #print('avg_loss = ',sum(batch_loss) / len(batch_loss))\n",
    "            #print('KLD : ',kl_divergence)\n",
    "            #print ('lgstd, meansq, logstdexp', self.model.logstd, self.model.mean.pow(2), self.model.logstd.exp())\n",
    "            #except:\n",
    "            #    print('weird thing happening')\n",
    "            #    pass\n",
    "           \n",
    "            \n",
    "        #print('loss : ', loss)\n",
    "        #loss_mse = nn.MSELoss()\n",
    "        #output_mse = loss_mse(A_pred.view(-1), labels.view(-1))\n",
    "        #output_mse.backward()\n",
    "        #self.optim.step()\n",
    "        # last layer:\n",
    "        #x = self.last_layer(x, eyes)\n",
    "\n",
    "        # Mean pooling: We expect xTs is of shape (batch_size, seq_len, hidden_dim)\n",
    "        #x = x.sum(1) / x.shape[1]\n",
    "\n",
    "        # Propagate through the last layer:\n",
    "        #x = self.output_layer(x)\n",
    "\n",
    "        # x.shape should be (batch_size, num_of_classes).\n",
    "        # label.shape should be (batch_size).\n",
    "        \n",
    "        #loss_val = self.loss(x, labels.to(torch.float32))\n",
    "\n",
    "        # Do the backward step:\n",
    "        #loss_val.backward()\n",
    "\n",
    "        # And the step \n",
    "        #self.optim.step()\n",
    "\n",
    "        #print('loss_mse : ', output_mse)\n",
    "        return batch_loss, kl_divergence\n",
    "\n",
    "    def predict(self, batch, debug=False, custom_vect_input=None):\n",
    "        lambs, poss, texts, labels, lens, _, lemmas = batch[:7]\n",
    "\n",
    "        # Adjacency in the first matrix are identity matrices:\n",
    "        eyes = make_eyes(lambs, lens)\n",
    "\n",
    "        # Set on cuda\n",
    "        if self.cuda:\n",
    "            lambs = lambs.cuda()\n",
    "            texts = texts.cuda()\n",
    "            labels = labels.cuda()\n",
    "            eyes = eyes.cuda()\n",
    "            poss = poss.cuda()\n",
    "            lemmas = lemmas.cuda()\n",
    "        if custom_vect_input is None:\n",
    "            # Embedding layers\n",
    "            const_word_vec = self.embedding(texts)\n",
    "            word_vec = self.word_embedding(texts)\n",
    "            pos_vec = self.pos_embedding(poss)\n",
    "            lemma_vec = self.lemma_embedding(lemmas)\n",
    "\n",
    "            x = torch.cat([word_vec, pos_vec, const_word_vec, lemma_vec], dim=2)\n",
    "        else:\n",
    "            x = custom_vect_input\n",
    "\n",
    "        # first layer:\n",
    "        # Adjacency matrix is the identity matrix.\n",
    "        x = self.first_layer(x, eyes)\n",
    "\n",
    "        # MPNN model\n",
    "        x = self.mp_model(x, lambs)\n",
    "\n",
    "        # last layer:\n",
    "        x = self.last_layer(x, eyes)\n",
    "\n",
    "        # Mean pooling: We expect xTs is of shape (batch_size, seq_len, hidden_dim)\n",
    "        x = x.sum(1) / x.shape[1]\n",
    "\n",
    "        # Propagate through the last layer:\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def load(self, filename, cuda=False):\n",
    "        if cuda:\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        try:\n",
    "            checkpoint = torch.load(filename + 'model_params.pt', map_location=device)\n",
    "        except BaseException:\n",
    "            print(\"Cannot load model from {}\".format(filename + 'model_params.pt'))\n",
    "            exit()\n",
    "\n",
    "        self.embedding.load_state_dict(checkpoint['embedding'])\n",
    "        self.pos_embedding.load_state_dict(checkpoint['pos_embedding'])\n",
    "        self.word_embedding.load_state_dict(checkpoint['word_embedding'])\n",
    "        self.lemma_embedding.load_state_dict(checkpoint['lemma_embedding'])\n",
    "        self.mp_model.load_state_dict(checkpoint['mp_model'])\n",
    "        #self.output_layer.load_state_dict(checkpoint['output_layer'])\n",
    "        #self.last_layer.load_state_dict(checkpoint['last_layer'])\n",
    "        #self.first_layer.load_state_dict(checkpoint['first_layer'])\n",
    "\n",
    "        self.vocab.load(filename + 'vocab.p')\n",
    "        self.pos_vocab.load(filename + 'pos_vocab.p')\n",
    "        self.lemma_vocab.load(filename + 'lemma_vocab.p')\n",
    "\n",
    "        self.vocab.word_embed = self.embedding\n",
    "        self.pos_vocab.word_embed = self.pos_embedding\n",
    "\n",
    "    def save(self, filename):\n",
    "        params = {\n",
    "            'embedding': self.embedding.state_dict(),\n",
    "            'pos_embedding': self.pos_embedding.state_dict(),\n",
    "            'mp_model': self.mp_model.state_dict(),\n",
    "            #'output_layer': self.output_layer.state_dict(),\n",
    "            #'last_layer': self.last_layer.state_dict(),\n",
    "            #'first_layer': self.first_layer.state_dict(),\n",
    "            'word_embedding': self.word_embedding.state_dict(),\n",
    "            'lemma_embedding': self.lemma_embedding.state_dict()\n",
    "        }\n",
    "        try:\n",
    "            torch.save(params, filename + 'model_params.pt')\n",
    "            self.vocab.save(filename + 'vocab.p')\n",
    "            self.pos_vocab.save(filename + 'pos_vocab.p')\n",
    "            self.lemma_vocab.save(filename + 'lemma_vocab.p')\n",
    "\n",
    "        except BaseException:\n",
    "            print(\"[Warning: Saving failed... continuing anyway.]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "mp_trainer = MLM(cfg, cuda=cfg['cuda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0:\n",
      "avg_loss =  -3.4394219908863306\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.52334155151303\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.573347682064022\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.93643584999729\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.029027151736067\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.96181920600094\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.9276060004481\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.990412848219677\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.937152252475592\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.883581381498236\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.888640397949701\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.901487842565591\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.84630291839034\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.818600436966273\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.712425595759133\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.810867179931444\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.832280912468089\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.871622021462273\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.863248384530731\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.843724972144923\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.836279357423898\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.931775136382975\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.026220679007446\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.010959751036655\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.01637751363126\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.021414600891622\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.00153877523154\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.0074519618250575\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.98132589585122\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -5.939831863930724\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.232335997505574\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.261559677033961\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.251762684710931\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.238712084429794\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.204007407173272\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -6.194291735039956\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "avg_loss =  -8588207061.982968\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/51 [19:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-531ee3d4d4f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-7a968c07b328>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;31m#kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*self.model.logstd - self.model.mean**2 - torch.exp(self.model.logstd)**2).sum(1).mean()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = cfg['epochs']\n",
    "for epoch in tqdm(range(cfg['epochs'] + 1)):\n",
    "    if epoch > int(epochs*0.8):\n",
    "        for g in mp_trainer.optim.param_groups:\n",
    "            g['lr'] = g['lr']/10\n",
    "\n",
    "\n",
    "    if epoch == range(cfg['epochs']):\n",
    "        # It's the last epoch, don't train again.\n",
    "        break\n",
    "\n",
    "    print('Train epoch {}:'.format(epoch))\n",
    "    epoch_loss = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        loss,kl = mp_trainer.update(batch)\n",
    "        epoch_loss += loss\n",
    "        if i % 10 == 0:\n",
    "            print('avg_loss = ',sum(epoch_loss) / len(epoch_loss))\n",
    "            print(kl)\n",
    "    #performance['loss'].append(np.mean(np.array(epoch_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, filename='', load=False, threshold=5):\n",
    "        if load:\n",
    "            assert os.path.exists(filename), \"Vocab file does not exist at \" + filename\n",
    "\n",
    "            self.id2word, self.word2id = self.load(filename)\n",
    "            self.size = len(self.id2word)\n",
    "            self.threshold = threshold\n",
    "            self.wordCounter = None\n",
    "        else:\n",
    "            self.id2word, self.word2id = {}, {}\n",
    "            self.size = 0\n",
    "            self.threshold = threshold\n",
    "            # We always add some custom tokens into the vocabulary.\n",
    "            self.add_words(\n",
    "                {'<PAD>': float('inf'), '<UNK>': float('inf'),'<MSK>' : 103})\n",
    "        self.word_embed = None\n",
    "\n",
    "    def add_words(self, counterOfTokens):\n",
    "        for item, value in counterOfTokens.items():\n",
    "            if value >= self.threshold:\n",
    "                if item not in self.word2id:\n",
    "                    # add it to the vocab\n",
    "                    self.word2id[item] = self.size\n",
    "                    self.id2word[self.size] = item\n",
    "                    self.size += 1\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as infile:\n",
    "            id2word = pickle.load(infile)\n",
    "            word2id = {word:id for id, word in id2word.items()}\n",
    "            self.id2word, self.word2id = id2word, word2id\n",
    "            self.size = len(self.id2word)\n",
    "\n",
    "        return id2word, word2id\n",
    "\n",
    "    def save(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "           \n",
    "        with open(filename, 'wb') as outfile:\n",
    "            pickle.dump(self.id2word, outfile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def init_word_embed(self, cfg, cache_dir='datasets/.word_vectors_cache'):\n",
    "        if cfg['word_vectors'] == 'Word2Vec':\n",
    "            from torchnlp.word_to_vector import FastText\n",
    "            all_word_vector = FastText(language=cfg['language'], cache=cache_dir, aligned=True)\n",
    "        else:\n",
    "            raise NotImplementedError('No word_vectors found which are called {}.'.format(cfg['word_vectors']))\n",
    "\n",
    "        # The the vectors only correspond to lower character words:\n",
    "        all_words = [word.lower() for word in list(self.word2id.keys())]\n",
    "        weights = all_word_vector[all_words]\n",
    "        \n",
    "        word_embed = torch.nn.Embedding(*weights.shape, _weight=weights)\n",
    "        #if cfg['device'] == 'cuda':\n",
    "        #    word_embed.cuda()\n",
    "\n",
    "        self.word_embed = word_embed\n",
    "        self.embed_size = weights.shape[1]\n",
    "\n",
    "    def words2vecs(self, words: list):\n",
    "        if not self.word_embed:\n",
    "            raise AttributeError(\"The word embeddings aren't initialized yet.\")\n",
    "        else:\n",
    "            vecs = self.word_embed(torch.tensor(self.map(words), requires_grad=False))\n",
    "        return vecs\n",
    "\n",
    "    def one_hot_ids2vecs(self, ids):\n",
    "        vecs = self.word_embed(ids)\n",
    "        return vecs\n",
    "\n",
    "    def map(self, token_list):\n",
    "        \"\"\"\n",
    "        Map a list of tokens to their ids.\n",
    "        \"\"\"\n",
    "        return [self.word2id[w] if w in self.word2id else self.word2id['<UNK>'] for w in token_list]\n",
    "\n",
    "    def unmap(self, idx_list):\n",
    "        \"\"\"\n",
    "        Unmap ids back to tokens.\n",
    "        \"\"\"\n",
    "        return [self.id2word[idx] for idx in idx_list]\n",
    "    \n",
    "def get_pos_vocab():\n",
    "    \"\"\"\n",
    "    Function to set up a part of speech vocabulary handcrafed.\n",
    "    \"\"\"\n",
    "    pos_id2word = {0: '<PAD>', 1: '<UNK>', 2: 'DET', 3: 'PROPN', 4: 'VERB', 5: 'PART', 6: 'ADJ', 7: 'PUNCT', 8: 'CCONJ',\n",
    "                   9: 'ADP', 10: 'PRON', 11: 'NOUN', 12: 'ADV', 13: 'INTJ', 14: 'NUM', 15: 'X', 16: 'SYM'}\n",
    "    pos_word2id = {word: id for id, word in pos_id2word.items()}\n",
    "    pos_vocab = Vocab()\n",
    "    pos_vocab.id2word = pos_id2word\n",
    "    pos_vocab.word2id = pos_word2id\n",
    "    pos_vocab.size = len(pos_vocab.id2word)\n",
    "    \n",
    "    return pos_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load sst data\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SSTData(Dataset):\n",
    "    def __init__(self,\n",
    "                 sst_data,\n",
    "                 vocab,\n",
    "                 nlp,\n",
    "                 lemma_vocab,\n",
    "                 pos_vocab = None,\n",
    "                 self_loop=True):\n",
    "\n",
    "        self.lemma_vocab = lemma_vocab\n",
    "        self.self_loop = self_loop\n",
    "        \n",
    "        \n",
    "        #sst_data = [sample for sample in sst_data if sample['label'] != 'neutral']\n",
    "        self.sst_data = sst_data\n",
    "        #self.sentiment_vocab = {'negative': 0, 'positive': 1}\n",
    "        \n",
    "        # Add sentencizer in the nlp if not already in it:\n",
    "        if \"sentencizer\" not in nlp.pipe_names:\n",
    "            # sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "            nlp.add_pipe('sentencizer', first=True)\n",
    "        self.nlp = nlp\n",
    "\n",
    "        self.vocab = vocab\n",
    "        if pos_vocab is None:\n",
    "            self.pos_vocab = get_pos_vocab()\n",
    "        else:\n",
    "            self.pos_vocab = pos_vocab\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.sst_data[idx]\n",
    "        if len(txt) <64:\n",
    "            txt = txt + ' PAD'*(64-len(txt))\n",
    "            doc = self.nlp(txt)\n",
    "        elif len(txt) > 64:\n",
    "            txt = txt[:65]\n",
    "            \n",
    "        doc = self.nlp(txt)\n",
    "\n",
    "        token_ids = self.vocab.map([token.text for token in doc])\n",
    "        # create random array of floats in equal dimension to input_ids\n",
    "        rand = torch.rand(np.shape(vocab.map([token.text for token in doc])))\n",
    "        # where the random array is less than 0.15, we set true\n",
    "        mask_arr = rand < 0.15\n",
    "        # create selection from mask_arr\n",
    "        selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "        for i in selection: \n",
    "            token_ids[i] = 103\n",
    "            break\n",
    "            \n",
    "            \n",
    "        \n",
    "        # make lambda\n",
    "        adj, root_id = doc_to_adj(doc, directed=False, self_loop=self.self_loop)\n",
    "\n",
    "        lamb = adj\n",
    "\n",
    "        # normalize\n",
    "        denom = lamb.sum(1)\n",
    "        lamb /= denom\n",
    "\n",
    "        # make text indices\n",
    "        \n",
    "        \n",
    "        # make pos ids\n",
    "        pos_ids = self.pos_vocab.map([token.pos_ for token in doc])\n",
    "        for i in selection: \n",
    "            pos_ids[i] = 1\n",
    "            break\n",
    "\n",
    "        # make lemma ids\n",
    "        lemma_ids = self.lemma_vocab.map([token.lemma_ for token in doc])\n",
    "        for i in selection: \n",
    "            lemma_ids[i] = 103\n",
    "            break\n",
    "        # make label\n",
    "        label = self.vocab.map([token.text for token in doc])\n",
    "\n",
    "        return token_ids, pos_ids, lamb, label, root_id, lemma_ids\n",
    "\n",
    "    \n",
    "    def data_check(self):\n",
    "        for idx,sent in enumerate(self.sst_data):\n",
    "            if len(sent) < 10:\n",
    "                self.sst_data.remove(sent)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sst_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn_sentim(batch):\n",
    "    lens = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        lens.append(lamb.shape[1])\n",
    "\n",
    "    max_len = max(lens)\n",
    "    lambs = []\n",
    "    texts = []\n",
    "    poss = []\n",
    "    lemmas = []\n",
    "    labels = []\n",
    "    root_ids = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        # Big lamb\n",
    "        lamb_ = torch.zeros(1, max_len, max_len)\n",
    "        lamb_[0, :lamb.shape[0], :lamb.shape[1]] = lamb\n",
    "        lambs.append(lamb_)\n",
    "\n",
    "        # Big text\n",
    "        text_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        text_[0, :len(text)] = torch.tensor(text)\n",
    "        texts.append(text_)\n",
    "\n",
    "        # Big pos\n",
    "        pos_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        pos_[0, :len(pos)] = torch.tensor(pos)\n",
    "        poss.append(pos_)\n",
    "\n",
    "        # Big lemma\n",
    "        lemma_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        lemma_[0, :len(lemma)] = torch.tensor(lemma)\n",
    "        lemmas.append(lemma_)\n",
    "\n",
    "        # Big label:\n",
    "        label_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        label_[0, :len(label)] = torch.tensor(label)\n",
    "        labels.append(label_)\n",
    "\n",
    "        # Big root_id:\n",
    "        root_id_ = torch.ones(1) * root_id\n",
    "        root_ids.append(root_id_.long())\n",
    "\n",
    "    lambs = torch.cat(lambs, dim=0)\n",
    "    texts = torch.cat(texts, dim=0)\n",
    "    poss = torch.cat(poss, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    root_ids = torch.cat(root_ids, dim=0)\n",
    "    lemmas = torch.cat(lemmas, dim=0)\n",
    "\n",
    "    return lambs, poss, texts, labels, lens, root_ids, lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def doc_to_adj(sent, directed=True, self_loop=False):\n",
    "    # Sent should be a spacy document. Can also be longer than a sentence.\n",
    "    sent_len = len(sent)\n",
    "    root_id = 1\n",
    "    ret = torch.zeros(sent_len, sent_len, dtype=torch.float32)\n",
    "\n",
    "    for token in sent:\n",
    "        for child in token.children:\n",
    "            if child.i >= sent_len:\n",
    "                #print('Something goes wrong here.')\n",
    "                print(child.i, sent_len, sent, token.i, token)\n",
    "                pass\n",
    "            ret[token.i, child.i] = 1\n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_id = token.i\n",
    "\n",
    "    if not directed:\n",
    "        ret = ret + ret.transpose(0, 1)\n",
    "\n",
    "    if self_loop:\n",
    "        for i in range(sent_len):\n",
    "            ret[i, i] = 1\n",
    "        \n",
    "    return ret, root_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchnlp.datasets import smt_dataset\n",
    "from torchtext.datasets import WikiText103\n",
    "import click\n",
    "\n",
    "import spacy\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"repo_name\": \"saved_models/sst_model1+/\", \n",
    "       \"epochs\": 50, \"optimizer\": \"adam\", \n",
    "       \"cuda\": False, \n",
    "       \"lr\": 0.0002, \n",
    "       \"num_of_layer\": 3, \n",
    "       \"hidden_dimension\": 10, \n",
    "       \"batch_size\": 1,\n",
    "       \"word_vectors\": \"Word2Vec\",\n",
    "       \"bias\": False, \n",
    "       \"pos_emb_dim\": 30,\n",
    "       \"model_type\": \"gcn\",\n",
    "       \"data_amount\": 1.0, \n",
    "       \"data_set\": \"sst\", \n",
    "       \"word_emb_dim\": 70,\n",
    "       \"lemma_emb_dim\": 50, \n",
    "       \"trainerfilename\": \"saved_models/sst_model1+/\",\n",
    "       \"logfilename\": \"saved_models/sst_model1+/log.txt\",\n",
    "       \"num_of_classes\": 64,\n",
    "       \"language\": \"en\",\n",
    "       \"normalize_lamb\": True,\n",
    "       \"laplacian\": False,\n",
    "       \"input_dimension\": 300,\n",
    "       'load_model' : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "train_data = Path('datasets/wikitext-103/wiki.train.tokens').read_text(encoding='utf-8')\n",
    "val_data = Path('datasets/wikitext-103/wiki.valid.tokens').read_text(encoding='utf-8')\n",
    "test_data = Path('datasets/wikitext-103/wiki.test.tokens').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out train headings and articles\n",
    "train_split = re.split(heading_pattern, train_data)\n",
    "train_headings = [x[7:-7] for x in train_split[1::2]]\n",
    "train_articles = [x for x in train_split[2::2]]\n",
    "\n",
    "# Split out validation headings and articles\n",
    "val_split = re.split(heading_pattern, val_data)\n",
    "val_headings = [x[7:-7] for x in val_split[1::2]]\n",
    "val_articles = [x for x in val_split[2::2]]\n",
    "\n",
    "# Split out test headings and articles\n",
    "test_split = re.split(heading_pattern, test_data)\n",
    "test_headings = [x[7:-7] for x in test_split[1::2]]\n",
    "test_articles = [x for x in test_split[2::2]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [i.split('. ') for i in train_articles]\n",
    "val_data = [i.split('. ') for i in val_articles]\n",
    "test_data = [i.split('. ') for i in test_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [item for sublist in test_data for item in sublist]\n",
    "val_data = [item for sublist in val_data for item in sublist]\n",
    "train_data = [item for sublist in train_data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "data_amount = .05\n",
    "\n",
    "print('Loading data...')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "cw = Counter()\n",
    "cl = Counter()\n",
    "\n",
    "\n",
    "\n",
    "train_set, test_set = train_data, val_data\n",
    "\n",
    "# only use the the percentage of data we want:\n",
    "train_set = train_set[:int(len(train_set) * data_amount)]\n",
    "test_set = test_set[:int(len(test_set) * data_amount)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Tower Building of the Little Rock Arsenal , also known as U.S',\n",
       " 'Arsenal Building , is a building located in MacArthur Park in downtown Little Rock , Arkansas ',\n",
       " \"Built in 1840 , it was part of Little Rock 's first military installation \",\n",
       " 'Since its decommissioning , The Tower Building has housed two museums ',\n",
       " 'It was home to the Arkansas Museum of Natural History and Antiquities from 1942 to 1997 and the MacArthur Museum of Arkansas Military History since 2001 ',\n",
       " 'It has also been the headquarters of the Little Rock sthetic Club since 1894 ',\n",
       " '\\n The building receives its name from its distinct octagonal tower ',\n",
       " 'Besides being the last remaining structure of the original Little Rock Arsenal and one of the oldest buildings in central Arkansas , it was also the birthplace of General Douglas MacArthur , who became the supreme commander of US forces in the South Pacific during World War II ',\n",
       " 'It was also the starting place of the Camden Expedition ',\n",
       " 'In 2011 it was named as one of the top 10 attractions in the state of Arkansas by <unk> \\n \\n = = Construction = = \\n \\n The arsenal was constructed at the request of Governor James Sevier Conway in response to the perceived dangers of frontier life and fears of the many Native Americans who were passing through the state on their way to the newly established Oklahoma Territory ']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BUILD VOCAB\n",
    "# # Count words:\n",
    "# for sample in train_set + test_set: cw += Counter([token.text for token in nlp(sample)])\n",
    "# # Count lemma:\n",
    "# for sample in train_set + test_set: cl += Counter([token.lemma_ for token in nlp(sample)])\n",
    "\n",
    "# vocab = Vocab()\n",
    "# lemma_vocab = Vocab()\n",
    "\n",
    "\n",
    "# # prepare vocab\n",
    "# vocab.add_words(cw)\n",
    "# #cfg['input_dimension'] = 300\n",
    "\n",
    "# lemma_vocab.add_words(cl)\n",
    "# pos_vocab = get_pos_vocab()\n",
    "\n",
    "# Save the parameter:\n",
    "#with open(repo_name + 'config.json', 'w') as fp:\n",
    "#    json.dump(cfg.get_as_dict(), fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '<PAD>',\n",
       "  1: '<UNK>',\n",
       "  2: '<MSK>',\n",
       "  3: 'the',\n",
       "  4: 'Tower',\n",
       "  5: 'Building',\n",
       "  6: 'of',\n",
       "  7: 'Little',\n",
       "  8: 'Rock',\n",
       "  9: 'Arsenal',\n",
       "  10: ',',\n",
       "  11: 'also',\n",
       "  12: 'know',\n",
       "  13: 'as',\n",
       "  14: 'U.S',\n",
       "  15: 'be',\n",
       "  16: 'a',\n",
       "  17: 'building',\n",
       "  18: 'locate',\n",
       "  19: 'in',\n",
       "  20: 'MacArthur',\n",
       "  21: 'Park',\n",
       "  22: 'downtown',\n",
       "  23: 'Arkansas',\n",
       "  24: 'build',\n",
       "  25: '1840',\n",
       "  26: 'it',\n",
       "  27: 'part',\n",
       "  28: \"'s\",\n",
       "  29: 'first',\n",
       "  30: 'military',\n",
       "  31: 'installation',\n",
       "  32: 'since',\n",
       "  33: 'its',\n",
       "  34: 'decommissioning',\n",
       "  35: 'have',\n",
       "  36: 'house',\n",
       "  37: 'two',\n",
       "  38: 'museum',\n",
       "  39: 'home',\n",
       "  40: 'to',\n",
       "  41: 'Museum',\n",
       "  42: 'Natural',\n",
       "  43: 'History',\n",
       "  44: 'and',\n",
       "  45: 'Antiquities',\n",
       "  46: 'from',\n",
       "  47: '1942',\n",
       "  48: '1997',\n",
       "  49: 'Military',\n",
       "  50: '2001',\n",
       "  51: 'headquarters',\n",
       "  52: 'Club',\n",
       "  53: '1894',\n",
       "  54: '\\n ',\n",
       "  55: 'receive',\n",
       "  56: 'name',\n",
       "  57: 'distinct',\n",
       "  58: 'octagonal',\n",
       "  59: 'tower',\n",
       "  60: 'besides',\n",
       "  61: 'last',\n",
       "  62: 'remaining',\n",
       "  63: 'structure',\n",
       "  64: 'original',\n",
       "  65: 'one',\n",
       "  66: 'old',\n",
       "  67: 'central',\n",
       "  68: 'birthplace',\n",
       "  69: 'General',\n",
       "  70: 'Douglas',\n",
       "  71: 'who',\n",
       "  72: 'become',\n",
       "  73: 'supreme',\n",
       "  74: 'commander',\n",
       "  75: 'US',\n",
       "  76: 'force',\n",
       "  77: 'South',\n",
       "  78: 'Pacific',\n",
       "  79: 'during',\n",
       "  80: 'World',\n",
       "  81: 'War',\n",
       "  82: 'II',\n",
       "  83: 'start',\n",
       "  84: 'place',\n",
       "  85: 'Camden',\n",
       "  86: 'Expedition',\n",
       "  87: '2011',\n",
       "  88: 'top',\n",
       "  89: '10',\n",
       "  90: 'attraction',\n",
       "  91: 'state',\n",
       "  92: 'by',\n",
       "  93: '<',\n",
       "  94: 'unk',\n",
       "  95: '>',\n",
       "  96: '\\n \\n ',\n",
       "  97: '=',\n",
       "  98: 'construction',\n",
       "  99: 'arsenal',\n",
       "  100: 'construct',\n",
       "  101: 'at',\n",
       "  102: 'request',\n",
       "  103: 'Governor',\n",
       "  104: 'James',\n",
       "  105: 'Sevier',\n",
       "  106: 'Conway',\n",
       "  107: 'response',\n",
       "  108: 'perceive',\n",
       "  109: 'danger',\n",
       "  110: 'frontier',\n",
       "  111: 'life',\n",
       "  112: 'fear',\n",
       "  113: 'many',\n",
       "  114: 'Native',\n",
       "  115: 'Americans',\n",
       "  116: 'pass',\n",
       "  117: 'through',\n",
       "  118: 'on',\n",
       "  119: 'their',\n",
       "  120: 'way',\n",
       "  121: 'newly',\n",
       "  122: 'establish',\n",
       "  123: 'Oklahoma',\n",
       "  124: 'Territory',\n",
       "  125: 'thirty',\n",
       "  126: '@-@',\n",
       "  127: 'six',\n",
       "  128: 'acre',\n",
       "  129: 'appropriate',\n",
       "  130: 'outskirt',\n",
       "  131: 'little',\n",
       "  132: 'Major',\n",
       "  133: 'Robert',\n",
       "  134: 'B',\n",
       "  135: 'Lee',\n",
       "  136: 'army',\n",
       "  137: 'land',\n",
       "  138: 'previously',\n",
       "  139: 'use',\n",
       "  140: 'racetrack',\n",
       "  141: 'local',\n",
       "  142: 'jockey',\n",
       "  143: 'club',\n",
       "  144: 'John',\n",
       "  145: 'Walker',\n",
       "  146: 'builder',\n",
       "  147: 'for',\n",
       "  148: 'Federal',\n",
       "  149: 'Government',\n",
       "  150: 'supervise',\n",
       "  151: 'originally',\n",
       "  152: '$',\n",
       "  153: '14',\n",
       "  154: '@,@',\n",
       "  155: '000',\n",
       "  156: 'allocate',\n",
       "  157: 'but',\n",
       "  158: 'prove',\n",
       "  159: 'inadequate',\n",
       "  160: 'budget',\n",
       "  161: 'later',\n",
       "  162: 'increase',\n",
       "  163: '30',\n",
       "  164: 'work',\n",
       "  165: 'begin',\n",
       "  166: 'permanent',\n",
       "  167: 'store',\n",
       "  168: 'ammunition',\n",
       "  169: 'design',\n",
       "  170: 'with',\n",
       "  171: '3',\n",
       "  172: 'foot',\n",
       "  173: 'thick',\n",
       "  174: '(',\n",
       "  175: '0',\n",
       "  176: '@.@',\n",
       "  177: '91',\n",
       "  178: 'm',\n",
       "  179: ')',\n",
       "  180: 'exterior',\n",
       "  181: 'wall',\n",
       "  182: 'plan',\n",
       "  183: 'call',\n",
       "  184: 'stone',\n",
       "  185: 'however',\n",
       "  186: 'masonry',\n",
       "  187: 'instead',\n",
       "  188: 'Gazette',\n",
       "  189: 'refer',\n",
       "  190: '\"',\n",
       "  191: 'splendid',\n",
       "  192: 'speciman',\n",
       "  193: 'civil',\n",
       "  194: 'several',\n",
       "  195: 'year',\n",
       "  196: 'which',\n",
       "  197: 'own',\n",
       "  198: 'federal',\n",
       "  199: 'government',\n",
       "  200: 'serve',\n",
       "  201: 'simple',\n",
       "  202: 'arm',\n",
       "  203: 'depot',\n",
       "  204: 'staff',\n",
       "  205: 'only',\n",
       "  206: 'handful',\n",
       "  207: 'soldier',\n",
       "  208: 'November',\n",
       "  209: '1860',\n",
       "  210: 'American',\n",
       "  211: 'Civil',\n",
       "  212: 'horizon',\n",
       "  213: 'company',\n",
       "  214: 'Second',\n",
       "  215: 'United',\n",
       "  216: 'States',\n",
       "  217: 'Artillery',\n",
       "  218: 'consist',\n",
       "  219: 'sixty',\n",
       "  220: 'five',\n",
       "  221: 'man',\n",
       "  222: 'transfer',\n",
       "  223: 'under',\n",
       "  224: 'command',\n",
       "  225: 'Captain',\n",
       "  226: 'January',\n",
       "  227: '15',\n",
       "  228: '1861',\n",
       "  229: 'legislature',\n",
       "  230: 'decide',\n",
       "  231: 'hold',\n",
       "  232: 'referendum',\n",
       "  233: 'determine',\n",
       "  234: 'if',\n",
       "  235: 'convention',\n",
       "  236: 'should',\n",
       "  237: 'consider',\n",
       "  238: 'issue',\n",
       "  239: 'secession',\n",
       "  240: 'elect',\n",
       "  241: 'delegate',\n",
       "  242: 'such',\n",
       "  243: 'February',\n",
       "  244: '18',\n",
       "  245: ';',\n",
       "  246: 'event',\n",
       "  247: 'would',\n",
       "  248: 'not',\n",
       "  249: 'wait',\n",
       "  250: '28',\n",
       "  251: 'then',\n",
       "  252: 'Henry',\n",
       "  253: 'Massey',\n",
       "  254: 'Rector',\n",
       "  255: 'inform',\n",
       "  256: 'that',\n",
       "  257: 'he',\n",
       "  258: 'his',\n",
       "  259: 'permit',\n",
       "  260: 'remain',\n",
       "  261: 'possession',\n",
       "  262: 'officer',\n",
       "  263: 'until',\n",
       "  264: 'State',\n",
       "  265: 'authority',\n",
       "  266: 'people',\n",
       "  267: 'shall',\n",
       "  268: 'sever',\n",
       "  269: 'connection',\n",
       "  270: 'respond',\n",
       "  271: 'this',\n",
       "  272: 'tell',\n",
       "  273: 'order',\n",
       "  274: 'come',\n",
       "  275: 'desperate',\n",
       "  276: 'ultimately',\n",
       "  277: 'futile',\n",
       "  278: 'dispatch',\n",
       "  279: 'letter',\n",
       "  280: 'telegram',\n",
       "  281: 'ask',\n",
       "  282: 'reinforcement',\n",
       "  283: 'although',\n",
       "  284: 'rumor',\n",
       "  285: 'widely',\n",
       "  286: 'spread',\n",
       "  287: 'they',\n",
       "  288: 'already',\n",
       "  289: 'telegraph',\n",
       "  290: 'wire',\n",
       "  291: 'span',\n",
       "  292: 'between',\n",
       "  293: 'Memphis',\n",
       "  294: 'recently',\n",
       "  295: 'complete',\n",
       "  296: 'attorney',\n",
       "  297: 'M',\n",
       "  298: 'compose',\n",
       "  299: 'capital',\n",
       "  300: 'message',\n",
       "  301: 'report',\n",
       "  302: 'unconfirmed',\n",
       "  303: 'more',\n",
       "  304: 'troop',\n",
       "  305: 'send',\n",
       "  306: 'reinforce',\n",
       "  307: 'outpost',\n",
       "  308: 'western',\n",
       "  309: 'indian',\n",
       "  310: 'nation',\n",
       "  311: 'all',\n",
       "  312: 'recall',\n",
       "  313: 'winter',\n",
       "  314: 'quarter',\n",
       "  315: 'garrison',\n",
       "  316: 'Fort',\n",
       "  317: 'Smith',\n",
       "  318: 'city',\n",
       "  319: 'rich',\n",
       "  320: 'depository',\n",
       "  321: 'suppose',\n",
       "  322: 'ultimate',\n",
       "  323: 'destination',\n",
       "  324: '[',\n",
       "  325: 'sic',\n",
       "  326: ']',\n",
       "  327: 'Telegram',\n",
       "  328: '31',\n",
       "  329: 'item',\n",
       "  330: 'intend',\n",
       "  331: 'simply',\n",
       "  332: 'piece',\n",
       "  333: 'news',\n",
       "  334: 'line',\n",
       "  335: 'quickly',\n",
       "  336: 'throughout',\n",
       "  337: 'fuel',\n",
       "  338: 'procession',\n",
       "  339: 'sentiment',\n",
       "  340: 'interpret',\n",
       "  341: 'some',\n",
       "  342: 'governor',\n",
       "  343: 'assemble',\n",
       "  344: 'help',\n",
       "  345: 'expel',\n",
       "  346: '5',\n",
       "  347: 'militia',\n",
       "  348: 'unit',\n",
       "  349: '1',\n",
       "  350: 'guarantee',\n",
       "  351: 'number',\n",
       "  352: 'could',\n",
       "  353: 'situation',\n",
       "  354: 'deem',\n",
       "  355: 'necessary',\n",
       "  356: 'vehemently',\n",
       "  357: 'deny',\n",
       "  358: 'or',\n",
       "  359: 'give',\n",
       "  360: 'any',\n",
       "  361: 'face',\n",
       "  362: 'fact',\n",
       "  363: 'believe',\n",
       "  364: 'follow',\n",
       "  365: 'consensus',\n",
       "  366: 'citizen',\n",
       "  367: 'against',\n",
       "  368: 'armed',\n",
       "  369: 'conflict',\n",
       "  370: 'civilian',\n",
       "  371: 'take',\n",
       "  372: 'control',\n",
       "  373: '6',\n",
       "  374: 'formal',\n",
       "  375: 'demand',\n",
       "  376: 'surrender',\n",
       "  377: 'movement',\n",
       "  378: 'prompt',\n",
       "  379: 'feeling',\n",
       "  380: 'pervade',\n",
       "  381: 'present',\n",
       "  382: 'emergency',\n",
       "  383: 'munition',\n",
       "  384: 'war',\n",
       "  385: 'security',\n",
       "  386: 'authorize',\n",
       "  387: 'I',\n",
       "  388: 'assume',\n",
       "  389: 'an',\n",
       "  390: 'aspect',\n",
       "  391: 'my',\n",
       "  392: 'duty',\n",
       "  393: 'executive',\n",
       "  394: 'interpose',\n",
       "  395: 'official',\n",
       "  396: 'prevent',\n",
       "  397: 'collision',\n",
       "  398: 'your',\n",
       "  399: 'therefore',\n",
       "  400: 'delivery',\n",
       "  401: 'charge',\n",
       "  402: 'subject',\n",
       "  403: 'action',\n",
       "  404: '4th',\n",
       "  405: 'March',\n",
       "  406: 'next',\n",
       "  407: 'perhaps',\n",
       "  408: 'because',\n",
       "  409: 'Abraham',\n",
       "  410: 'Lincoln',\n",
       "  411: 'yet',\n",
       "  412: 'inaugurate',\n",
       "  413: 'President',\n",
       "  414: 'no',\n",
       "  415: 'instruction',\n",
       "  416: 'superior',\n",
       "  417: 'withdraw',\n",
       "  418: 'agree',\n",
       "  419: 'long',\n",
       "  420: 'three',\n",
       "  421: 'provision',\n",
       "  422: ':',\n",
       "  423: 'allow',\n",
       "  424: 'safe',\n",
       "  425: 'passage',\n",
       "  426: 'direction',\n",
       "  427: 'carry',\n",
       "  428: 'personal',\n",
       "  429: 'public',\n",
       "  430: 'property',\n",
       "  431: 'march',\n",
       "  432: 'away',\n",
       "  433: 'leave',\n",
       "  434: 'conquer',\n",
       "  435: 'morning',\n",
       "  436: '8',\n",
       "  437: 'sign',\n",
       "  438: 'agreement',\n",
       "  439: 'hand',\n",
       "  440: 'afternoon',\n",
       "  441: 'head',\n",
       "  442: 'point',\n",
       "  443: 'except',\n",
       "  444: 'stay',\n",
       "  445: 'behind',\n",
       "  446: 'listen',\n",
       "  447: 'speech',\n",
       "  448: 'over',\n",
       "  449: 'person',\n",
       "  450: 'classify',\n",
       "  451: 'deposit',\n",
       "  452: 'mean',\n",
       "  453: 'warehouse',\n",
       "  454: 'storage',\n",
       "  455: 'weapon',\n",
       "  456: 'time',\n",
       "  457: 'crisis',\n",
       "  458: 'thus',\n",
       "  459: 'there',\n",
       "  460: 'substantial',\n",
       "  461: 'operation',\n",
       "  462: 'ordnance',\n",
       "  463: 'fabrication',\n",
       "  464: 'repair',\n",
       "  465: 'nor',\n",
       "  466: 'manufacture',\n",
       "  467: 'cartridge',\n",
       "  468: 'fall',\n",
       "  469: 'into',\n",
       "  470: 'Most',\n",
       "  471: 'these',\n",
       "  472: 'scratch',\n",
       "  473: 'effort',\n",
       "  474: 'Board',\n",
       "  475: 'inside',\n",
       "  476: 'after',\n",
       "  477: 'seizure',\n",
       "  478: 'Confederates',\n",
       "  479: '247',\n",
       "  480: '250',\n",
       "  481: 'musket',\n",
       "  482: '520',\n",
       "  483: 'percussion',\n",
       "  484: 'cap',\n",
       "  485: 'well',\n",
       "  486: 'four',\n",
       "  487: 'bronze',\n",
       "  488: 'cannon',\n",
       "  489: 'battery',\n",
       "  490: 'inventory',\n",
       "  491: 'cal',\n",
       "  492: '625',\n",
       "  493: 'convert',\n",
       "  494: '53',\n",
       "  495: 'smoothbore',\n",
       "  496: '357',\n",
       "  497: 'rifle',\n",
       "  498: '900',\n",
       "  499: 'common',\n",
       "  500: '125',\n",
       "  501: 'Mississippi',\n",
       "  502: 'Rifle',\n",
       "  503: '54',\n",
       "  504: '2',\n",
       "  505: 'Hall',\n",
       "  506: 'carbine',\n",
       "  507: '267',\n",
       "  508: '864',\n",
       "  509: 'Total',\n",
       "  510: 'approximately',\n",
       "  511: 'serviceable',\n",
       "  512: 'ready',\n",
       "  513: 'note',\n",
       "  514: '364',\n",
       "  515: 'available',\n",
       "  516: 'disposition',\n",
       "  517: 'find',\n",
       "  518: 'somewhat',\n",
       "  519: 'sketchy',\n",
       "  520: 'various',\n",
       "  521: 'record',\n",
       "  522: 'can',\n",
       "  523: 'surmise',\n",
       "  524: '5th',\n",
       "  525: '6th',\n",
       "  526: '7th',\n",
       "  527: '8th',\n",
       "  528: 'Infantry',\n",
       "  529: 'Regiments',\n",
       "  530: 'muster',\n",
       "  531: 'June',\n",
       "  532: '/',\n",
       "  533: 'caliber',\n",
       "  534: '9th',\n",
       "  535: '10th',\n",
       "  536: 'Kelly',\n",
       "  537: 'Battalion',\n",
       "  538: '3rd',\n",
       "  539: 'Cavalry',\n",
       "  540: 'Regiment',\n",
       "  541: 'Rifles',\n",
       "  542: 'comprise',\n",
       "  543: 'infantry',\n",
       "  544: 'Van',\n",
       "  545: 'Dorn',\n",
       "  546: 'Army',\n",
       "  547: 'West',\n",
       "  548: '1st',\n",
       "  549: '2nd',\n",
       "  550: 'Mounted',\n",
       "  551: '11th',\n",
       "  552: '12th',\n",
       "  553: 'supply',\n",
       "  554: 'almost',\n",
       "  555: 'completely',\n",
       "  556: 'exhausted',\n",
       "  557: 'equipment',\n",
       "  558: 'machinery',\n",
       "  559: 'remove',\n",
       "  560: 'east',\n",
       "  561: 'River',\n",
       "  562: 'Maj',\n",
       "  563: 'Gen',\n",
       "  564: 'Earl',\n",
       "  565: 'April',\n",
       "  566: 'May',\n",
       "  567: '1862',\n",
       "  568: 'accountability',\n",
       "  569: 'lose',\n",
       "  570: 'you',\n",
       "  571: 'appearance',\n",
       "  572: 'down',\n",
       "  573: 'river',\n",
       "  574: 'Napoleon',\n",
       "  575: 'Jackson',\n",
       "  576: 'where',\n",
       "  577: 'probably',\n",
       "  578: 'destroy',\n",
       "  579: 'Vicksburg',\n",
       "  580: 'campaign',\n",
       "  581: 'early',\n",
       "  582: 'summer',\n",
       "  583: '1863',\n",
       "  584: 'Thomas',\n",
       "  585: 'C',\n",
       "  586: 'Hindman',\n",
       "  587: 'district',\n",
       "  588: 'nearly',\n",
       "  589: 'destitute',\n",
       "  590: 'material',\n",
       "  591: 'another',\n",
       "  592: 'armory',\n",
       "  593: 'revive',\n",
       "  594: 'collection',\n",
       "  595: 'armament',\n",
       "  596: 'small',\n",
       "  597: 'make',\n",
       "  598: 'both',\n",
       "  599: 'turn',\n",
       "  600: 'out',\n",
       "  601: 'quantity',\n",
       "  602: 'excellent',\n",
       "  603: 'quality',\n",
       "  604: 'lead',\n",
       "  605: 'mine',\n",
       "  606: 'open',\n",
       "  607: 'chemical',\n",
       "  608: 'laboratory',\n",
       "  609: 'successfully',\n",
       "  610: 'operate',\n",
       "  611: 'aid',\n",
       "  612: 'Ordnance',\n",
       "  613: 'Department',\n",
       "  614: 'oil',\n",
       "  615: 'spirit',\n",
       "  616: 'iron',\n",
       "  617: 'other',\n",
       "  618: 'valuable',\n",
       "  619: 'medicine',\n",
       "  620: 'near',\n",
       "  621: '75',\n",
       "  622: 'mile',\n",
       "  623: 'south',\n",
       "  624: 'tool',\n",
       "  625: 'gather',\n",
       "  626: 'piecemeal',\n",
       "  627: 'else',\n",
       "  628: 'labor',\n",
       "  629: 'nothing',\n",
       "  630: 'sort',\n",
       "  631: 'before',\n",
       "  632: 'attempt',\n",
       "  633: 'account',\n",
       "  634: 'knowledge',\n",
       "  635: 'neither',\n",
       "  636: 'sufficient',\n",
       "  637: 'enterprise',\n",
       "  638: 'among',\n",
       "  639: 'engage',\n",
       "  640: 'undertaking',\n",
       "  641: 'further',\n",
       "  642: 'along',\n",
       "  643: 'procure',\n",
       "  644: 'vicinity',\n",
       "  645: 'donation',\n",
       "  646: 'purchase',\n",
       "  647: 'bring',\n",
       "  648: 'rapidly',\n",
       "  649: 'prepare',\n",
       "  650: 'Laboratory',\n",
       "  651: 'purpose',\n",
       "  652: 'illustrate',\n",
       "  653: 'pitiful',\n",
       "  654: 'scarcity',\n",
       "  655: 'country',\n",
       "  656: 'may',\n",
       "  657: 'document',\n",
       "  658: 'Library',\n",
       "  659: 'paper',\n",
       "  660: 'employ',\n",
       "  661: 'conscript',\n",
       "  662: 'impressed',\n",
       "  663: 'damage',\n",
       "  664: 'gun',\n",
       "  665: 'about',\n",
       "  666: 'equal',\n",
       "  667: 'commence',\n",
       "  668: 'once',\n",
       "  669: 'inspect',\n",
       "  670: 'observe',\n",
       "  671: '500',\n",
       "  672: 'strong',\n",
       "  673: 'Fitch',\n",
       "  674: 'remainder',\n",
       "  675: '-',\n",
       "  676: '1500',\n",
       "  677: \"'\",\n",
       "  678: 'l',\n",
       "  679: 'rust',\n",
       "  680: 'soon',\n",
       "  681: 'shotgun',\n",
       "  682: 'obtain',\n",
       "  683: 'pike',\n",
       "  684: 'lance',\n",
       "  685: 'most',\n",
       "  686: 'day',\n",
       "  687: 'elapse',\n",
       "  688: 'change',\n",
       "  689: 'effect',\n",
       "  690: 'confederate',\n",
       "  691: 'establishment',\n",
       "  692: 'reactivate',\n",
       "  693: 'August',\n",
       "  694: 'look',\n",
       "  695: 'around',\n",
       "  696: 'suitable',\n",
       "  697: 'activity',\n",
       "  698: 'Confederate',\n",
       "  699: 'Navy',\n",
       "  700: 'borrow',\n",
       "  701: 'Lieutenant',\n",
       "  702: 'W',\n",
       "  703: 'Lt',\n",
       "  704: 'gunboat',\n",
       "  705: 'hope',\n",
       "  706: 'ironclad',\n",
       "  707: 'select',\n",
       "  708: 'continue',\n",
       "  709: 'draw',\n",
       "  710: 'pay',\n",
       "  711: 'include',\n",
       "  712: 'artillery',\n",
       "  713: 'function',\n",
       "  714: 'rank',\n",
       "  715: 'lieutenant',\n",
       "  716: 'colonel',\n",
       "  717: 'lt',\n",
       "  718: 'Col',\n",
       "  719: 'return',\n",
       "  720: 'month',\n",
       "  721: 'Vol',\n",
       "  722: '149',\n",
       "  723: 'Chapter',\n",
       "  724: 'IV',\n",
       "  725: 'Rebel',\n",
       "  726: 'Records',\n",
       "  727: 'scope',\n",
       "  728: 'crucial',\n",
       "  729: 'accord',\n",
       "  730: 'when',\n",
       "  731: 'Post',\n",
       "  732: 'shop',\n",
       "  733: 'fabricate',\n",
       "  734: 'etc',\n",
       "  735: 'employment',\n",
       "  736: 'laborer',\n",
       "  737: 'himself',\n",
       "  738: 'green',\n",
       "  739: 'Murphy',\n",
       "  740: 'addition',\n",
       "  741: '20',\n",
       "  742: 'enlist',\n",
       "  743: 'foreman',\n",
       "  744: 'clerk',\n",
       "  745: '26',\n",
       "  746: 'carpenter',\n",
       "  747: 'packing',\n",
       "  748: 'box',\n",
       "  749: 'following',\n",
       "  750: 'perform',\n",
       "  751: 'pair',\n",
       "  752: 'bullet',\n",
       "  753: 'mould',\n",
       "  754: 'buck',\n",
       "  755: '&',\n",
       "  756: 'ball',\n",
       "  757: 'shot',\n",
       "  758: '750',\n",
       "  759: 'guard',\n",
       "  760: 'office',\n",
       "  761: 'police',\n",
       "  762: 'post',\n",
       "  763: 'up',\n",
       "  764: 'Sanford',\n",
       "  765: 'Faulkner',\n",
       "  766: 'composer',\n",
       "  767: 'Traveler',\n",
       "  768: 'presumably',\n",
       "  769: 'naval',\n",
       "  770: 'Work',\n",
       "  771: 'Done',\n",
       "  772: 'show',\n",
       "  773: 'flint',\n",
       "  774: '275',\n",
       "  775: 'fuze',\n",
       "  776: '117',\n",
       "  777: 'round',\n",
       "  778: 'pounder',\n",
       "  779: 'canister',\n",
       "  780: 'shoot',\n",
       "  781: '130',\n",
       "  782: '96',\n",
       "  783: '236',\n",
       "  784: 'mostly',\n",
       "  785: 'service',\n",
       "  786: '23',\n",
       "  787: 'pistol',\n",
       "  788: '752',\n",
       "  789: 'package',\n",
       "  790: 'paint',\n",
       "  791: '4',\n",
       "  792: 'carriage',\n",
       "  793: 'Guard',\n",
       "  794: 'above',\n",
       "  795: 'those',\n",
       "  796: 'standard',\n",
       "  797: 'indicate',\n",
       "  798: 'predominant',\n",
       "  799: 'sixth',\n",
       "  800: 'still',\n",
       "  801: 'less',\n",
       "  802: 'than',\n",
       "  803: 'obsolete',\n",
       "  804: 'do',\n",
       "  805: 'same',\n",
       "  806: 'pace',\n",
       "  807: 'scale',\n",
       "  808: 'ominous',\n",
       "  809: 'notation',\n",
       "  810: 'week',\n",
       "  811: 'pack',\n",
       "  812: 'obedience',\n",
       "  813: 'Chief',\n",
       "  814: 'District',\n",
       "  815: 'mark',\n",
       "  816: 'beginning',\n",
       "  817: 'evacuation',\n",
       "  818: 'advance',\n",
       "  819: 'Frederick',\n",
       "  820: 'Steele',\n",
       "  821: 'September',\n",
       "  822: '11',\n",
       "  823: '1864',\n",
       "  824: 'Union',\n",
       "  825: 'recapture',\n",
       "  826: 'briefly',\n",
       "  827: 'seize',\n",
       "  828: 'Joseph',\n",
       "  829: 'Brooks',\n",
       "  830: 'loyalist',\n",
       "  831: 'Baxter',\n",
       "  832: '1874',\n",
       "  833: 'decommission',\n",
       "  834: '1873',\n",
       "  835: 'rename',\n",
       "  836: 'Barracks',\n",
       "  837: 'barracks',\n",
       "  838: 'married',\n",
       "  839: 'family',\n",
       "  840: 'drastically',\n",
       "  841: 'alter',\n",
       "  842: 'outside',\n",
       "  843: 'prior',\n",
       "  844: 'renovation',\n",
       "  845: 'rear',\n",
       "  846: 'basement',\n",
       "  847: 'door',\n",
       "  848: 'provide',\n",
       "  849: 'entrance',\n",
       "  850: 'while',\n",
       "  851: 'hoist',\n",
       "  852: 'move',\n",
       "  853: 'floor',\n",
       "  854: '1868',\n",
       "  855: 'front',\n",
       "  856: 'porch',\n",
       "  857: 'add',\n",
       "  858: 'interior',\n",
       "  859: 'stair',\n",
       "  860: 'today',\n",
       "  861: 'staircase',\n",
       "  862: '1880',\n",
       "  863: 'bear',\n",
       "  864: 'northwest',\n",
       "  865: 'upper',\n",
       "  866: 'father',\n",
       "  867: 'Arthur',\n",
       "  868: 'station',\n",
       "  869: 'close',\n",
       "  870: 'favor',\n",
       "  871: 'railroad',\n",
       "  872: 'quick',\n",
       "  873: 'deployment',\n",
       "  874: 'word',\n",
       "  875: 'Washington',\n",
       "  876: 'site',\n",
       "  877: 'must',\n",
       "  878: 'abandon',\n",
       "  879: 'October',\n",
       "  880: '1890',\n",
       "  881: '12',\n",
       "  882: '1893',\n",
       "  883: 'surround',\n",
       "  884: 'trade',\n",
       "  885: 'km',\n",
       "  886: '',\n",
       "  887: 'North',\n",
       "  888: 'condition',\n",
       "  889: 'forever',\n",
       "  890: 'exclusively',\n",
       "  891: 'devoted',\n",
       "  892: 'park',\n",
       "  893: 'Big',\n",
       "  894: 'Mountain',\n",
       "  895: 'north',\n",
       "  896: 'side',\n",
       "  897: 'Logan',\n",
       "  898: 'H',\n",
       "  899: 'root',\n",
       "  900: 'demolish',\n",
       "  901: 'woman',\n",
       "  902: 'society',\n",
       "  903: 'west',\n",
       "  904: 'due',\n",
       "  905: 'membership',\n",
       "  906: 'need',\n",
       "  907: 'large',\n",
       "  908: 'previous',\n",
       "  909: 'member',\n",
       "  910: 'organization',\n",
       "  911: 'raise',\n",
       "  912: 'money',\n",
       "  913: 'furnish',\n",
       "  914: 'Columbian',\n",
       "  915: 'Exposition',\n",
       "  916: 'Chicago',\n",
       "  917: 'Fair',\n",
       "  918: 'fair',\n",
       "  919: 'conclusion',\n",
       "  920: 'artifact',\n",
       "  921: 'exhibit',\n",
       "  922: 'display',\n",
       "  923: 'invite',\n",
       "  924: 'meet',\n",
       "  925: 'Room',\n",
       "  926: 'meeting',\n",
       "  927: 'largely',\n",
       "  928: 'unoccupied',\n",
       "  929: 'fifty',\n",
       "  930: 'suffer',\n",
       "  931: 'significant',\n",
       "  932: 'deterioration',\n",
       "  933: 'much',\n",
       "  934: 'financial',\n",
       "  935: 'support',\n",
       "  936: 'period',\n",
       "  937: 'even',\n",
       "  938: 'electric',\n",
       "  939: 'bill',\n",
       "  940: 'Great',\n",
       "  941: 'Depression',\n",
       "  942: 'headquarter',\n",
       "  943: '20th',\n",
       "  944: 'century',\n",
       "  945: 'Veterans',\n",
       "  946: 'Reunion',\n",
       "  947: '',\n",
       "  948: '1911',\n",
       "  949: '106',\n",
       "  950: 'veteran',\n",
       "  951: 'popular',\n",
       "  952: 'gathering',\n",
       "  953: 'history',\n",
       "  954: 'attend',\n",
       "  955: 'camp',\n",
       "  956: 'camping',\n",
       "  957: 'area',\n",
       "  958: 'National',\n",
       "  959: '1912',\n",
       "  960: 'second',\n",
       "  961: 'library',\n",
       "  962: '1917',\n",
       "  963: 'fire',\n",
       "  964: 'now',\n",
       "  965: 'go',\n",
       "  966: 'band',\n",
       "  967: 'shell',\n",
       "  968: 'h',\n",
       "  969: 'Foster',\n",
       "  970: 'exist',\n",
       "  971: '1936',\n",
       "  972: 'Works',\n",
       "  973: 'Progress',\n",
       "  974: 'Administration',\n",
       "  975: 'Fine',\n",
       "  976: 'Arts',\n",
       "  977: 'Center',\n",
       "  978: 'just',\n",
       "  979: 'list',\n",
       "  980: 'Register',\n",
       "  981: 'Historic',\n",
       "  982: 'Places',\n",
       "  983: '1970',\n",
       "  984: 'association',\n",
       "  985: 'Sites',\n",
       "  986: 'Landmark',\n",
       "  987: 'designate',\n",
       "  988: '1994',\n",
       "  989: 'renovate',\n",
       "  990: 'philanthropist',\n",
       "  991: 'new',\n",
       "  992: 'City',\n",
       "  993: 'Bernie',\n",
       "  994: 'Babcock',\n",
       "  995: 'finally',\n",
       "  996: 'honor',\n",
       "  997: 'Science',\n",
       "  998: 'merge',\n",
       "  999: 'Children',\n",
       "  ...},\n",
       " {'<PAD>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<MSK>': 2,\n",
       "  'the': 3,\n",
       "  'Tower': 4,\n",
       "  'Building': 5,\n",
       "  'of': 6,\n",
       "  'Little': 7,\n",
       "  'Rock': 8,\n",
       "  'Arsenal': 9,\n",
       "  ',': 10,\n",
       "  'also': 11,\n",
       "  'know': 12,\n",
       "  'as': 13,\n",
       "  'U.S': 14,\n",
       "  'be': 15,\n",
       "  'a': 16,\n",
       "  'building': 17,\n",
       "  'locate': 18,\n",
       "  'in': 19,\n",
       "  'MacArthur': 20,\n",
       "  'Park': 21,\n",
       "  'downtown': 22,\n",
       "  'Arkansas': 23,\n",
       "  'build': 24,\n",
       "  '1840': 25,\n",
       "  'it': 26,\n",
       "  'part': 27,\n",
       "  \"'s\": 28,\n",
       "  'first': 29,\n",
       "  'military': 30,\n",
       "  'installation': 31,\n",
       "  'since': 32,\n",
       "  'its': 33,\n",
       "  'decommissioning': 34,\n",
       "  'have': 35,\n",
       "  'house': 36,\n",
       "  'two': 37,\n",
       "  'museum': 38,\n",
       "  'home': 39,\n",
       "  'to': 40,\n",
       "  'Museum': 41,\n",
       "  'Natural': 42,\n",
       "  'History': 43,\n",
       "  'and': 44,\n",
       "  'Antiquities': 45,\n",
       "  'from': 46,\n",
       "  '1942': 47,\n",
       "  '1997': 48,\n",
       "  'Military': 49,\n",
       "  '2001': 50,\n",
       "  'headquarters': 51,\n",
       "  'Club': 52,\n",
       "  '1894': 53,\n",
       "  '\\n ': 54,\n",
       "  'receive': 55,\n",
       "  'name': 56,\n",
       "  'distinct': 57,\n",
       "  'octagonal': 58,\n",
       "  'tower': 59,\n",
       "  'besides': 60,\n",
       "  'last': 61,\n",
       "  'remaining': 62,\n",
       "  'structure': 63,\n",
       "  'original': 64,\n",
       "  'one': 65,\n",
       "  'old': 66,\n",
       "  'central': 67,\n",
       "  'birthplace': 68,\n",
       "  'General': 69,\n",
       "  'Douglas': 70,\n",
       "  'who': 71,\n",
       "  'become': 72,\n",
       "  'supreme': 73,\n",
       "  'commander': 74,\n",
       "  'US': 75,\n",
       "  'force': 76,\n",
       "  'South': 77,\n",
       "  'Pacific': 78,\n",
       "  'during': 79,\n",
       "  'World': 80,\n",
       "  'War': 81,\n",
       "  'II': 82,\n",
       "  'start': 83,\n",
       "  'place': 84,\n",
       "  'Camden': 85,\n",
       "  'Expedition': 86,\n",
       "  '2011': 87,\n",
       "  'top': 88,\n",
       "  '10': 89,\n",
       "  'attraction': 90,\n",
       "  'state': 91,\n",
       "  'by': 92,\n",
       "  '<': 93,\n",
       "  'unk': 94,\n",
       "  '>': 95,\n",
       "  '\\n \\n ': 96,\n",
       "  '=': 97,\n",
       "  'construction': 98,\n",
       "  'arsenal': 99,\n",
       "  'construct': 100,\n",
       "  'at': 101,\n",
       "  'request': 102,\n",
       "  'Governor': 103,\n",
       "  'James': 104,\n",
       "  'Sevier': 105,\n",
       "  'Conway': 106,\n",
       "  'response': 107,\n",
       "  'perceive': 108,\n",
       "  'danger': 109,\n",
       "  'frontier': 110,\n",
       "  'life': 111,\n",
       "  'fear': 112,\n",
       "  'many': 113,\n",
       "  'Native': 114,\n",
       "  'Americans': 115,\n",
       "  'pass': 116,\n",
       "  'through': 117,\n",
       "  'on': 118,\n",
       "  'their': 119,\n",
       "  'way': 120,\n",
       "  'newly': 121,\n",
       "  'establish': 122,\n",
       "  'Oklahoma': 123,\n",
       "  'Territory': 124,\n",
       "  'thirty': 125,\n",
       "  '@-@': 126,\n",
       "  'six': 127,\n",
       "  'acre': 128,\n",
       "  'appropriate': 129,\n",
       "  'outskirt': 130,\n",
       "  'little': 131,\n",
       "  'Major': 132,\n",
       "  'Robert': 133,\n",
       "  'B': 134,\n",
       "  'Lee': 135,\n",
       "  'army': 136,\n",
       "  'land': 137,\n",
       "  'previously': 138,\n",
       "  'use': 139,\n",
       "  'racetrack': 140,\n",
       "  'local': 141,\n",
       "  'jockey': 142,\n",
       "  'club': 143,\n",
       "  'John': 144,\n",
       "  'Walker': 145,\n",
       "  'builder': 146,\n",
       "  'for': 147,\n",
       "  'Federal': 148,\n",
       "  'Government': 149,\n",
       "  'supervise': 150,\n",
       "  'originally': 151,\n",
       "  '$': 152,\n",
       "  '14': 153,\n",
       "  '@,@': 154,\n",
       "  '000': 155,\n",
       "  'allocate': 156,\n",
       "  'but': 157,\n",
       "  'prove': 158,\n",
       "  'inadequate': 159,\n",
       "  'budget': 160,\n",
       "  'later': 161,\n",
       "  'increase': 162,\n",
       "  '30': 163,\n",
       "  'work': 164,\n",
       "  'begin': 165,\n",
       "  'permanent': 166,\n",
       "  'store': 167,\n",
       "  'ammunition': 168,\n",
       "  'design': 169,\n",
       "  'with': 170,\n",
       "  '3': 171,\n",
       "  'foot': 172,\n",
       "  'thick': 173,\n",
       "  '(': 174,\n",
       "  '0': 175,\n",
       "  '@.@': 176,\n",
       "  '91': 177,\n",
       "  'm': 178,\n",
       "  ')': 179,\n",
       "  'exterior': 180,\n",
       "  'wall': 181,\n",
       "  'plan': 182,\n",
       "  'call': 183,\n",
       "  'stone': 184,\n",
       "  'however': 185,\n",
       "  'masonry': 186,\n",
       "  'instead': 187,\n",
       "  'Gazette': 188,\n",
       "  'refer': 189,\n",
       "  '\"': 190,\n",
       "  'splendid': 191,\n",
       "  'speciman': 192,\n",
       "  'civil': 193,\n",
       "  'several': 194,\n",
       "  'year': 195,\n",
       "  'which': 196,\n",
       "  'own': 197,\n",
       "  'federal': 198,\n",
       "  'government': 199,\n",
       "  'serve': 200,\n",
       "  'simple': 201,\n",
       "  'arm': 202,\n",
       "  'depot': 203,\n",
       "  'staff': 204,\n",
       "  'only': 205,\n",
       "  'handful': 206,\n",
       "  'soldier': 207,\n",
       "  'November': 208,\n",
       "  '1860': 209,\n",
       "  'American': 210,\n",
       "  'Civil': 211,\n",
       "  'horizon': 212,\n",
       "  'company': 213,\n",
       "  'Second': 214,\n",
       "  'United': 215,\n",
       "  'States': 216,\n",
       "  'Artillery': 217,\n",
       "  'consist': 218,\n",
       "  'sixty': 219,\n",
       "  'five': 220,\n",
       "  'man': 221,\n",
       "  'transfer': 222,\n",
       "  'under': 223,\n",
       "  'command': 224,\n",
       "  'Captain': 225,\n",
       "  'January': 226,\n",
       "  '15': 227,\n",
       "  '1861': 228,\n",
       "  'legislature': 229,\n",
       "  'decide': 230,\n",
       "  'hold': 231,\n",
       "  'referendum': 232,\n",
       "  'determine': 233,\n",
       "  'if': 234,\n",
       "  'convention': 235,\n",
       "  'should': 236,\n",
       "  'consider': 237,\n",
       "  'issue': 238,\n",
       "  'secession': 239,\n",
       "  'elect': 240,\n",
       "  'delegate': 241,\n",
       "  'such': 242,\n",
       "  'February': 243,\n",
       "  '18': 244,\n",
       "  ';': 245,\n",
       "  'event': 246,\n",
       "  'would': 247,\n",
       "  'not': 248,\n",
       "  'wait': 249,\n",
       "  '28': 250,\n",
       "  'then': 251,\n",
       "  'Henry': 252,\n",
       "  'Massey': 253,\n",
       "  'Rector': 254,\n",
       "  'inform': 255,\n",
       "  'that': 256,\n",
       "  'he': 257,\n",
       "  'his': 258,\n",
       "  'permit': 259,\n",
       "  'remain': 260,\n",
       "  'possession': 261,\n",
       "  'officer': 262,\n",
       "  'until': 263,\n",
       "  'State': 264,\n",
       "  'authority': 265,\n",
       "  'people': 266,\n",
       "  'shall': 267,\n",
       "  'sever': 268,\n",
       "  'connection': 269,\n",
       "  'respond': 270,\n",
       "  'this': 271,\n",
       "  'tell': 272,\n",
       "  'order': 273,\n",
       "  'come': 274,\n",
       "  'desperate': 275,\n",
       "  'ultimately': 276,\n",
       "  'futile': 277,\n",
       "  'dispatch': 278,\n",
       "  'letter': 279,\n",
       "  'telegram': 280,\n",
       "  'ask': 281,\n",
       "  'reinforcement': 282,\n",
       "  'although': 283,\n",
       "  'rumor': 284,\n",
       "  'widely': 285,\n",
       "  'spread': 286,\n",
       "  'they': 287,\n",
       "  'already': 288,\n",
       "  'telegraph': 289,\n",
       "  'wire': 290,\n",
       "  'span': 291,\n",
       "  'between': 292,\n",
       "  'Memphis': 293,\n",
       "  'recently': 294,\n",
       "  'complete': 295,\n",
       "  'attorney': 296,\n",
       "  'M': 297,\n",
       "  'compose': 298,\n",
       "  'capital': 299,\n",
       "  'message': 300,\n",
       "  'report': 301,\n",
       "  'unconfirmed': 302,\n",
       "  'more': 303,\n",
       "  'troop': 304,\n",
       "  'send': 305,\n",
       "  'reinforce': 306,\n",
       "  'outpost': 307,\n",
       "  'western': 308,\n",
       "  'indian': 309,\n",
       "  'nation': 310,\n",
       "  'all': 311,\n",
       "  'recall': 312,\n",
       "  'winter': 313,\n",
       "  'quarter': 314,\n",
       "  'garrison': 315,\n",
       "  'Fort': 316,\n",
       "  'Smith': 317,\n",
       "  'city': 318,\n",
       "  'rich': 319,\n",
       "  'depository': 320,\n",
       "  'suppose': 321,\n",
       "  'ultimate': 322,\n",
       "  'destination': 323,\n",
       "  '[': 324,\n",
       "  'sic': 325,\n",
       "  ']': 326,\n",
       "  'Telegram': 327,\n",
       "  '31': 328,\n",
       "  'item': 329,\n",
       "  'intend': 330,\n",
       "  'simply': 331,\n",
       "  'piece': 332,\n",
       "  'news': 333,\n",
       "  'line': 334,\n",
       "  'quickly': 335,\n",
       "  'throughout': 336,\n",
       "  'fuel': 337,\n",
       "  'procession': 338,\n",
       "  'sentiment': 339,\n",
       "  'interpret': 340,\n",
       "  'some': 341,\n",
       "  'governor': 342,\n",
       "  'assemble': 343,\n",
       "  'help': 344,\n",
       "  'expel': 345,\n",
       "  '5': 346,\n",
       "  'militia': 347,\n",
       "  'unit': 348,\n",
       "  '1': 349,\n",
       "  'guarantee': 350,\n",
       "  'number': 351,\n",
       "  'could': 352,\n",
       "  'situation': 353,\n",
       "  'deem': 354,\n",
       "  'necessary': 355,\n",
       "  'vehemently': 356,\n",
       "  'deny': 357,\n",
       "  'or': 358,\n",
       "  'give': 359,\n",
       "  'any': 360,\n",
       "  'face': 361,\n",
       "  'fact': 362,\n",
       "  'believe': 363,\n",
       "  'follow': 364,\n",
       "  'consensus': 365,\n",
       "  'citizen': 366,\n",
       "  'against': 367,\n",
       "  'armed': 368,\n",
       "  'conflict': 369,\n",
       "  'civilian': 370,\n",
       "  'take': 371,\n",
       "  'control': 372,\n",
       "  '6': 373,\n",
       "  'formal': 374,\n",
       "  'demand': 375,\n",
       "  'surrender': 376,\n",
       "  'movement': 377,\n",
       "  'prompt': 378,\n",
       "  'feeling': 379,\n",
       "  'pervade': 380,\n",
       "  'present': 381,\n",
       "  'emergency': 382,\n",
       "  'munition': 383,\n",
       "  'war': 384,\n",
       "  'security': 385,\n",
       "  'authorize': 386,\n",
       "  'I': 387,\n",
       "  'assume': 388,\n",
       "  'an': 389,\n",
       "  'aspect': 390,\n",
       "  'my': 391,\n",
       "  'duty': 392,\n",
       "  'executive': 393,\n",
       "  'interpose': 394,\n",
       "  'official': 395,\n",
       "  'prevent': 396,\n",
       "  'collision': 397,\n",
       "  'your': 398,\n",
       "  'therefore': 399,\n",
       "  'delivery': 400,\n",
       "  'charge': 401,\n",
       "  'subject': 402,\n",
       "  'action': 403,\n",
       "  '4th': 404,\n",
       "  'March': 405,\n",
       "  'next': 406,\n",
       "  'perhaps': 407,\n",
       "  'because': 408,\n",
       "  'Abraham': 409,\n",
       "  'Lincoln': 410,\n",
       "  'yet': 411,\n",
       "  'inaugurate': 412,\n",
       "  'President': 413,\n",
       "  'no': 414,\n",
       "  'instruction': 415,\n",
       "  'superior': 416,\n",
       "  'withdraw': 417,\n",
       "  'agree': 418,\n",
       "  'long': 419,\n",
       "  'three': 420,\n",
       "  'provision': 421,\n",
       "  ':': 422,\n",
       "  'allow': 423,\n",
       "  'safe': 424,\n",
       "  'passage': 425,\n",
       "  'direction': 426,\n",
       "  'carry': 427,\n",
       "  'personal': 428,\n",
       "  'public': 429,\n",
       "  'property': 430,\n",
       "  'march': 431,\n",
       "  'away': 432,\n",
       "  'leave': 433,\n",
       "  'conquer': 434,\n",
       "  'morning': 435,\n",
       "  '8': 436,\n",
       "  'sign': 437,\n",
       "  'agreement': 438,\n",
       "  'hand': 439,\n",
       "  'afternoon': 440,\n",
       "  'head': 441,\n",
       "  'point': 442,\n",
       "  'except': 443,\n",
       "  'stay': 444,\n",
       "  'behind': 445,\n",
       "  'listen': 446,\n",
       "  'speech': 447,\n",
       "  'over': 448,\n",
       "  'person': 449,\n",
       "  'classify': 450,\n",
       "  'deposit': 451,\n",
       "  'mean': 452,\n",
       "  'warehouse': 453,\n",
       "  'storage': 454,\n",
       "  'weapon': 455,\n",
       "  'time': 456,\n",
       "  'crisis': 457,\n",
       "  'thus': 458,\n",
       "  'there': 459,\n",
       "  'substantial': 460,\n",
       "  'operation': 461,\n",
       "  'ordnance': 462,\n",
       "  'fabrication': 463,\n",
       "  'repair': 464,\n",
       "  'nor': 465,\n",
       "  'manufacture': 466,\n",
       "  'cartridge': 467,\n",
       "  'fall': 468,\n",
       "  'into': 469,\n",
       "  'Most': 470,\n",
       "  'these': 471,\n",
       "  'scratch': 472,\n",
       "  'effort': 473,\n",
       "  'Board': 474,\n",
       "  'inside': 475,\n",
       "  'after': 476,\n",
       "  'seizure': 477,\n",
       "  'Confederates': 478,\n",
       "  '247': 479,\n",
       "  '250': 480,\n",
       "  'musket': 481,\n",
       "  '520': 482,\n",
       "  'percussion': 483,\n",
       "  'cap': 484,\n",
       "  'well': 485,\n",
       "  'four': 486,\n",
       "  'bronze': 487,\n",
       "  'cannon': 488,\n",
       "  'battery': 489,\n",
       "  'inventory': 490,\n",
       "  'cal': 491,\n",
       "  '625': 492,\n",
       "  'convert': 493,\n",
       "  '53': 494,\n",
       "  'smoothbore': 495,\n",
       "  '357': 496,\n",
       "  'rifle': 497,\n",
       "  '900': 498,\n",
       "  'common': 499,\n",
       "  '125': 500,\n",
       "  'Mississippi': 501,\n",
       "  'Rifle': 502,\n",
       "  '54': 503,\n",
       "  '2': 504,\n",
       "  'Hall': 505,\n",
       "  'carbine': 506,\n",
       "  '267': 507,\n",
       "  '864': 508,\n",
       "  'Total': 509,\n",
       "  'approximately': 510,\n",
       "  'serviceable': 511,\n",
       "  'ready': 512,\n",
       "  'note': 513,\n",
       "  '364': 514,\n",
       "  'available': 515,\n",
       "  'disposition': 516,\n",
       "  'find': 517,\n",
       "  'somewhat': 518,\n",
       "  'sketchy': 519,\n",
       "  'various': 520,\n",
       "  'record': 521,\n",
       "  'can': 522,\n",
       "  'surmise': 523,\n",
       "  '5th': 524,\n",
       "  '6th': 525,\n",
       "  '7th': 526,\n",
       "  '8th': 527,\n",
       "  'Infantry': 528,\n",
       "  'Regiments': 529,\n",
       "  'muster': 530,\n",
       "  'June': 531,\n",
       "  '/': 532,\n",
       "  'caliber': 533,\n",
       "  '9th': 534,\n",
       "  '10th': 535,\n",
       "  'Kelly': 536,\n",
       "  'Battalion': 537,\n",
       "  '3rd': 538,\n",
       "  'Cavalry': 539,\n",
       "  'Regiment': 540,\n",
       "  'Rifles': 541,\n",
       "  'comprise': 542,\n",
       "  'infantry': 543,\n",
       "  'Van': 544,\n",
       "  'Dorn': 545,\n",
       "  'Army': 546,\n",
       "  'West': 547,\n",
       "  '1st': 548,\n",
       "  '2nd': 549,\n",
       "  'Mounted': 550,\n",
       "  '11th': 551,\n",
       "  '12th': 552,\n",
       "  'supply': 553,\n",
       "  'almost': 554,\n",
       "  'completely': 555,\n",
       "  'exhausted': 556,\n",
       "  'equipment': 557,\n",
       "  'machinery': 558,\n",
       "  'remove': 559,\n",
       "  'east': 560,\n",
       "  'River': 561,\n",
       "  'Maj': 562,\n",
       "  'Gen': 563,\n",
       "  'Earl': 564,\n",
       "  'April': 565,\n",
       "  'May': 566,\n",
       "  '1862': 567,\n",
       "  'accountability': 568,\n",
       "  'lose': 569,\n",
       "  'you': 570,\n",
       "  'appearance': 571,\n",
       "  'down': 572,\n",
       "  'river': 573,\n",
       "  'Napoleon': 574,\n",
       "  'Jackson': 575,\n",
       "  'where': 576,\n",
       "  'probably': 577,\n",
       "  'destroy': 578,\n",
       "  'Vicksburg': 579,\n",
       "  'campaign': 580,\n",
       "  'early': 581,\n",
       "  'summer': 582,\n",
       "  '1863': 583,\n",
       "  'Thomas': 584,\n",
       "  'C': 585,\n",
       "  'Hindman': 586,\n",
       "  'district': 587,\n",
       "  'nearly': 588,\n",
       "  'destitute': 589,\n",
       "  'material': 590,\n",
       "  'another': 591,\n",
       "  'armory': 592,\n",
       "  'revive': 593,\n",
       "  'collection': 594,\n",
       "  'armament': 595,\n",
       "  'small': 596,\n",
       "  'make': 597,\n",
       "  'both': 598,\n",
       "  'turn': 599,\n",
       "  'out': 600,\n",
       "  'quantity': 601,\n",
       "  'excellent': 602,\n",
       "  'quality': 603,\n",
       "  'lead': 604,\n",
       "  'mine': 605,\n",
       "  'open': 606,\n",
       "  'chemical': 607,\n",
       "  'laboratory': 608,\n",
       "  'successfully': 609,\n",
       "  'operate': 610,\n",
       "  'aid': 611,\n",
       "  'Ordnance': 612,\n",
       "  'Department': 613,\n",
       "  'oil': 614,\n",
       "  'spirit': 615,\n",
       "  'iron': 616,\n",
       "  'other': 617,\n",
       "  'valuable': 618,\n",
       "  'medicine': 619,\n",
       "  'near': 620,\n",
       "  '75': 621,\n",
       "  'mile': 622,\n",
       "  'south': 623,\n",
       "  'tool': 624,\n",
       "  'gather': 625,\n",
       "  'piecemeal': 626,\n",
       "  'else': 627,\n",
       "  'labor': 628,\n",
       "  'nothing': 629,\n",
       "  'sort': 630,\n",
       "  'before': 631,\n",
       "  'attempt': 632,\n",
       "  'account': 633,\n",
       "  'knowledge': 634,\n",
       "  'neither': 635,\n",
       "  'sufficient': 636,\n",
       "  'enterprise': 637,\n",
       "  'among': 638,\n",
       "  'engage': 639,\n",
       "  'undertaking': 640,\n",
       "  'further': 641,\n",
       "  'along': 642,\n",
       "  'procure': 643,\n",
       "  'vicinity': 644,\n",
       "  'donation': 645,\n",
       "  'purchase': 646,\n",
       "  'bring': 647,\n",
       "  'rapidly': 648,\n",
       "  'prepare': 649,\n",
       "  'Laboratory': 650,\n",
       "  'purpose': 651,\n",
       "  'illustrate': 652,\n",
       "  'pitiful': 653,\n",
       "  'scarcity': 654,\n",
       "  'country': 655,\n",
       "  'may': 656,\n",
       "  'document': 657,\n",
       "  'Library': 658,\n",
       "  'paper': 659,\n",
       "  'employ': 660,\n",
       "  'conscript': 661,\n",
       "  'impressed': 662,\n",
       "  'damage': 663,\n",
       "  'gun': 664,\n",
       "  'about': 665,\n",
       "  'equal': 666,\n",
       "  'commence': 667,\n",
       "  'once': 668,\n",
       "  'inspect': 669,\n",
       "  'observe': 670,\n",
       "  '500': 671,\n",
       "  'strong': 672,\n",
       "  'Fitch': 673,\n",
       "  'remainder': 674,\n",
       "  '-': 675,\n",
       "  '1500': 676,\n",
       "  \"'\": 677,\n",
       "  'l': 678,\n",
       "  'rust': 679,\n",
       "  'soon': 680,\n",
       "  'shotgun': 681,\n",
       "  'obtain': 682,\n",
       "  'pike': 683,\n",
       "  'lance': 684,\n",
       "  'most': 685,\n",
       "  'day': 686,\n",
       "  'elapse': 687,\n",
       "  'change': 688,\n",
       "  'effect': 689,\n",
       "  'confederate': 690,\n",
       "  'establishment': 691,\n",
       "  'reactivate': 692,\n",
       "  'August': 693,\n",
       "  'look': 694,\n",
       "  'around': 695,\n",
       "  'suitable': 696,\n",
       "  'activity': 697,\n",
       "  'Confederate': 698,\n",
       "  'Navy': 699,\n",
       "  'borrow': 700,\n",
       "  'Lieutenant': 701,\n",
       "  'W': 702,\n",
       "  'Lt': 703,\n",
       "  'gunboat': 704,\n",
       "  'hope': 705,\n",
       "  'ironclad': 706,\n",
       "  'select': 707,\n",
       "  'continue': 708,\n",
       "  'draw': 709,\n",
       "  'pay': 710,\n",
       "  'include': 711,\n",
       "  'artillery': 712,\n",
       "  'function': 713,\n",
       "  'rank': 714,\n",
       "  'lieutenant': 715,\n",
       "  'colonel': 716,\n",
       "  'lt': 717,\n",
       "  'Col': 718,\n",
       "  'return': 719,\n",
       "  'month': 720,\n",
       "  'Vol': 721,\n",
       "  '149': 722,\n",
       "  'Chapter': 723,\n",
       "  'IV': 724,\n",
       "  'Rebel': 725,\n",
       "  'Records': 726,\n",
       "  'scope': 727,\n",
       "  'crucial': 728,\n",
       "  'accord': 729,\n",
       "  'when': 730,\n",
       "  'Post': 731,\n",
       "  'shop': 732,\n",
       "  'fabricate': 733,\n",
       "  'etc': 734,\n",
       "  'employment': 735,\n",
       "  'laborer': 736,\n",
       "  'himself': 737,\n",
       "  'green': 738,\n",
       "  'Murphy': 739,\n",
       "  'addition': 740,\n",
       "  '20': 741,\n",
       "  'enlist': 742,\n",
       "  'foreman': 743,\n",
       "  'clerk': 744,\n",
       "  '26': 745,\n",
       "  'carpenter': 746,\n",
       "  'packing': 747,\n",
       "  'box': 748,\n",
       "  'following': 749,\n",
       "  'perform': 750,\n",
       "  'pair': 751,\n",
       "  'bullet': 752,\n",
       "  'mould': 753,\n",
       "  'buck': 754,\n",
       "  '&': 755,\n",
       "  'ball': 756,\n",
       "  'shot': 757,\n",
       "  '750': 758,\n",
       "  'guard': 759,\n",
       "  'office': 760,\n",
       "  'police': 761,\n",
       "  'post': 762,\n",
       "  'up': 763,\n",
       "  'Sanford': 764,\n",
       "  'Faulkner': 765,\n",
       "  'composer': 766,\n",
       "  'Traveler': 767,\n",
       "  'presumably': 768,\n",
       "  'naval': 769,\n",
       "  'Work': 770,\n",
       "  'Done': 771,\n",
       "  'show': 772,\n",
       "  'flint': 773,\n",
       "  '275': 774,\n",
       "  'fuze': 775,\n",
       "  '117': 776,\n",
       "  'round': 777,\n",
       "  'pounder': 778,\n",
       "  'canister': 779,\n",
       "  'shoot': 780,\n",
       "  '130': 781,\n",
       "  '96': 782,\n",
       "  '236': 783,\n",
       "  'mostly': 784,\n",
       "  'service': 785,\n",
       "  '23': 786,\n",
       "  'pistol': 787,\n",
       "  '752': 788,\n",
       "  'package': 789,\n",
       "  'paint': 790,\n",
       "  '4': 791,\n",
       "  'carriage': 792,\n",
       "  'Guard': 793,\n",
       "  'above': 794,\n",
       "  'those': 795,\n",
       "  'standard': 796,\n",
       "  'indicate': 797,\n",
       "  'predominant': 798,\n",
       "  'sixth': 799,\n",
       "  'still': 800,\n",
       "  'less': 801,\n",
       "  'than': 802,\n",
       "  'obsolete': 803,\n",
       "  'do': 804,\n",
       "  'same': 805,\n",
       "  'pace': 806,\n",
       "  'scale': 807,\n",
       "  'ominous': 808,\n",
       "  'notation': 809,\n",
       "  'week': 810,\n",
       "  'pack': 811,\n",
       "  'obedience': 812,\n",
       "  'Chief': 813,\n",
       "  'District': 814,\n",
       "  'mark': 815,\n",
       "  'beginning': 816,\n",
       "  'evacuation': 817,\n",
       "  'advance': 818,\n",
       "  'Frederick': 819,\n",
       "  'Steele': 820,\n",
       "  'September': 821,\n",
       "  '11': 822,\n",
       "  '1864': 823,\n",
       "  'Union': 824,\n",
       "  'recapture': 825,\n",
       "  'briefly': 826,\n",
       "  'seize': 827,\n",
       "  'Joseph': 828,\n",
       "  'Brooks': 829,\n",
       "  'loyalist': 830,\n",
       "  'Baxter': 831,\n",
       "  '1874': 832,\n",
       "  'decommission': 833,\n",
       "  '1873': 834,\n",
       "  'rename': 835,\n",
       "  'Barracks': 836,\n",
       "  'barracks': 837,\n",
       "  'married': 838,\n",
       "  'family': 839,\n",
       "  'drastically': 840,\n",
       "  'alter': 841,\n",
       "  'outside': 842,\n",
       "  'prior': 843,\n",
       "  'renovation': 844,\n",
       "  'rear': 845,\n",
       "  'basement': 846,\n",
       "  'door': 847,\n",
       "  'provide': 848,\n",
       "  'entrance': 849,\n",
       "  'while': 850,\n",
       "  'hoist': 851,\n",
       "  'move': 852,\n",
       "  'floor': 853,\n",
       "  '1868': 854,\n",
       "  'front': 855,\n",
       "  'porch': 856,\n",
       "  'add': 857,\n",
       "  'interior': 858,\n",
       "  'stair': 859,\n",
       "  'today': 860,\n",
       "  'staircase': 861,\n",
       "  '1880': 862,\n",
       "  'bear': 863,\n",
       "  'northwest': 864,\n",
       "  'upper': 865,\n",
       "  'father': 866,\n",
       "  'Arthur': 867,\n",
       "  'station': 868,\n",
       "  'close': 869,\n",
       "  'favor': 870,\n",
       "  'railroad': 871,\n",
       "  'quick': 872,\n",
       "  'deployment': 873,\n",
       "  'word': 874,\n",
       "  'Washington': 875,\n",
       "  'site': 876,\n",
       "  'must': 877,\n",
       "  'abandon': 878,\n",
       "  'October': 879,\n",
       "  '1890': 880,\n",
       "  '12': 881,\n",
       "  '1893': 882,\n",
       "  'surround': 883,\n",
       "  'trade': 884,\n",
       "  'km': 885,\n",
       "  '': 886,\n",
       "  'North': 887,\n",
       "  'condition': 888,\n",
       "  'forever': 889,\n",
       "  'exclusively': 890,\n",
       "  'devoted': 891,\n",
       "  'park': 892,\n",
       "  'Big': 893,\n",
       "  'Mountain': 894,\n",
       "  'north': 895,\n",
       "  'side': 896,\n",
       "  'Logan': 897,\n",
       "  'H': 898,\n",
       "  'root': 899,\n",
       "  'demolish': 900,\n",
       "  'woman': 901,\n",
       "  'society': 902,\n",
       "  'west': 903,\n",
       "  'due': 904,\n",
       "  'membership': 905,\n",
       "  'need': 906,\n",
       "  'large': 907,\n",
       "  'previous': 908,\n",
       "  'member': 909,\n",
       "  'organization': 910,\n",
       "  'raise': 911,\n",
       "  'money': 912,\n",
       "  'furnish': 913,\n",
       "  'Columbian': 914,\n",
       "  'Exposition': 915,\n",
       "  'Chicago': 916,\n",
       "  'Fair': 917,\n",
       "  'fair': 918,\n",
       "  'conclusion': 919,\n",
       "  'artifact': 920,\n",
       "  'exhibit': 921,\n",
       "  'display': 922,\n",
       "  'invite': 923,\n",
       "  'meet': 924,\n",
       "  'Room': 925,\n",
       "  'meeting': 926,\n",
       "  'largely': 927,\n",
       "  'unoccupied': 928,\n",
       "  'fifty': 929,\n",
       "  'suffer': 930,\n",
       "  'significant': 931,\n",
       "  'deterioration': 932,\n",
       "  'much': 933,\n",
       "  'financial': 934,\n",
       "  'support': 935,\n",
       "  'period': 936,\n",
       "  'even': 937,\n",
       "  'electric': 938,\n",
       "  'bill': 939,\n",
       "  'Great': 940,\n",
       "  'Depression': 941,\n",
       "  'headquarter': 942,\n",
       "  '20th': 943,\n",
       "  'century': 944,\n",
       "  'Veterans': 945,\n",
       "  'Reunion': 946,\n",
       "  '': 947,\n",
       "  '1911': 948,\n",
       "  '106': 949,\n",
       "  'veteran': 950,\n",
       "  'popular': 951,\n",
       "  'gathering': 952,\n",
       "  'history': 953,\n",
       "  'attend': 954,\n",
       "  'camp': 955,\n",
       "  'camping': 956,\n",
       "  'area': 957,\n",
       "  'National': 958,\n",
       "  '1912': 959,\n",
       "  'second': 960,\n",
       "  'library': 961,\n",
       "  '1917': 962,\n",
       "  'fire': 963,\n",
       "  'now': 964,\n",
       "  'go': 965,\n",
       "  'band': 966,\n",
       "  'shell': 967,\n",
       "  'h': 968,\n",
       "  'Foster': 969,\n",
       "  'exist': 970,\n",
       "  '1936': 971,\n",
       "  'Works': 972,\n",
       "  'Progress': 973,\n",
       "  'Administration': 974,\n",
       "  'Fine': 975,\n",
       "  'Arts': 976,\n",
       "  'Center': 977,\n",
       "  'just': 978,\n",
       "  'list': 979,\n",
       "  'Register': 980,\n",
       "  'Historic': 981,\n",
       "  'Places': 982,\n",
       "  '1970': 983,\n",
       "  'association': 984,\n",
       "  'Sites': 985,\n",
       "  'Landmark': 986,\n",
       "  'designate': 987,\n",
       "  '1994': 988,\n",
       "  'renovate': 989,\n",
       "  'philanthropist': 990,\n",
       "  'new': 991,\n",
       "  'City': 992,\n",
       "  'Bernie': 993,\n",
       "  'Babcock': 994,\n",
       "  'finally': 995,\n",
       "  'honor': 996,\n",
       "  'Science': 997,\n",
       "  'merge': 998,\n",
       "  'Children': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "lemma_vocab = Vocab()\n",
    "vocab.load('word_vocab_50.json')\n",
    "lemma_vocab.load('lemma_vocab_50.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56661"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data ...\n"
     ]
    }
   ],
   "source": [
    "print('Prepare data ...')\n",
    "\n",
    "\n",
    "train_data = SSTData(\n",
    "    train_set,\n",
    "    vocab,\n",
    "    nlp,\n",
    "    lemma_vocab,\n",
    ")\n",
    "test_data = SSTData(\n",
    "    test_set,\n",
    "    vocab,\n",
    "    nlp,\n",
    "    lemma_vocab,\n",
    ")\n",
    "\n",
    "cfg['vocab'] = vocab\n",
    "cfg['pos_vocab'] = lemma_vocab\n",
    "cfg['lemma_vocab'] = lemma_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194388"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190886"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = collate_fn_sentim\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size=32,\n",
    "                         collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(len(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(data_loader, mp_trainer, num_of_classes = 64):\n",
    "    # Evaluation\n",
    "    f1_micro = 0\n",
    "    prec_micro = 0\n",
    "    recall_micro = 0\n",
    "    accuracy = 0.\n",
    "    for batch in data_loader:\n",
    "        lambs, poss, texts, labels, lens = batch[:5]\n",
    "        logits = mp_trainer.predict(batch)\n",
    "\n",
    "        predicted_classes = torch.argmax(logits, dim=1)\n",
    "        if num_of_classes == 2:\n",
    "            average = 'binary'\n",
    "        else:\n",
    "            average = 'micro'\n",
    "        f1_micro += f1_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        prec_micro += precision_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        recall_micro += recall_score(labels.cpu(), predicted_classes.cpu(), average= average)\n",
    "        accuracy += accuracy_score(labels.cpu(), predicted_classes.cpu())\n",
    "\n",
    "    f1_micro /= len(data_loader)\n",
    "    prec_micro /= len(data_loader)\n",
    "    recall_micro /= len(data_loader)\n",
    "    accuracy /= len(data_loader)\n",
    "\n",
    "    return f1_micro, prec_micro, recall_micro, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'saved_models/sst_model1+/'\n",
    "cfg['trainerfilename'] = repo_name\n",
    "cfg['logfilename'] = repo_name + 'log.txt'\n",
    "\n",
    "# Task specific settings\n",
    "cfg['num_of_classes'] = 64\n",
    "cfg['language'] = 'en'\n",
    "\n",
    "# model specific settings\n",
    "cfg['normalize_lamb'] = True\n",
    "cfg['laplacian'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    }
   ],
   "source": [
    "with open(cfg['logfilename'], 'a+') as logfile:\n",
    "    logfile.write(\n",
    "        '{} training samples set into {} batches, {} test samples set into {} batches\\n'.format(len(train_data),\n",
    "                                                                                                len(train_loader),\n",
    "                                                                                                len(test_data),\n",
    "                                                                                                len(test_loader)))\n",
    "    logfile.write('Start training.\\n')\n",
    "\n",
    "all_losses = []\n",
    "performance = {'acc_test': [], 'prec_test': [], 'f1_test': [], 'recall_test': [],\n",
    "               'acc_train': [], 'prec_train': [], 'f1_train': [], 'recall_train': [],\n",
    "               'loss': []}\n",
    "print('Start training')\n",
    "best_accuracy = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "mp_trainer = MLM(cfg, cuda=cfg['cuda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|                                                                                           | 0/51 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-060c60656420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-0edb74e6b38d>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_word_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemma_vec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mA_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), \n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "epochs = cfg['epochs']\n",
    "for epoch in tqdm(range(cfg['epochs'] + 1)):\n",
    "    if epoch > int(epochs*0.8):\n",
    "        for g in mp_trainer.optim.param_groups:\n",
    "            g['lr'] = g['lr']/10\n",
    "\n",
    "#     f1_macro_test, prec_macro_test, recall_macro_test, accuracy_test = test_performance(test_loader,\n",
    "#                                                                                         mp_trainer,\n",
    "#                                                                                         num_of_classes = cfg['output_dimesion'])\n",
    "#     f1_macro_train, prec_macro_train, recall_macro_train, accuracy_train = test_performance(train_loader,\n",
    "#                                                                                             mp_trainer,\n",
    "#                                                                                             num_of_classes = cfg['output_dimesion'])\n",
    "#     # Save vals for test:\n",
    "#     performance['acc_test'].append(accuracy_test)\n",
    "#     performance['prec_test'].append(prec_macro_test)\n",
    "#     performance['f1_test'].append(f1_macro_test)\n",
    "#     performance['recall_test'].append(recall_macro_test)\n",
    "\n",
    "#     # Save vals for train:\n",
    "#     performance['acc_train'].append(accuracy_train)\n",
    "#     performance['prec_train'].append(prec_macro_train)\n",
    "#     performance['f1_train'].append(f1_macro_train)\n",
    "#     performance['recall_train'].append(recall_macro_train)\n",
    "\n",
    "#     with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#         logfile.write('On test we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_test,\n",
    "#                                                                                                 f1_macro_test,\n",
    "#                                                                                                 prec_macro_test,\n",
    "#                                                                                                 recall_macro_test))\n",
    "#         logfile.write('On train we have accuracy = {}, f1 = {}, prec = {}, recall = {}\\n'.format(accuracy_train,\n",
    "#                                                                                                 f1_macro_train,\n",
    "#                                                                                                 prec_macro_train,\n",
    "#                                                                                                 recall_macro_train))\n",
    "\n",
    "#     if accuracy_test > best_accuracy:\n",
    "#         mp_trainer.save(cfg['trainerfilename'])\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write(\n",
    "#                 'New best model with accuracy = {} saved at {}\\n'.format(accuracy_test, cfg['trainerfilename']))\n",
    "#         best_accuracy = accuracy_test\n",
    "#     else:\n",
    "#         with open(cfg['logfilename'], 'a+') as logfile:\n",
    "#             logfile.write('Best model has accuracy = {}\\n'.format(best_accuracy))\n",
    "\n",
    "#     # save current performance.\n",
    "#     json.dump(performance, open(repo_name + 'performance.json', 'w'))\n",
    "#     # plot performance\n",
    "#     visualize_performance(performance, repo_name)\n",
    "\n",
    "    if epoch == range(cfg['epochs']):\n",
    "        # It's the last epoch, don't train again.\n",
    "        break\n",
    "\n",
    "    print('Train epoch {}:'.format(epoch))\n",
    "    epoch_loss = []\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        loss = mp_trainer.update(batch)\n",
    "        epoch_loss.append(float(loss.detach()))\n",
    "        print(loss)\n",
    "        if i % 50 == 0:\n",
    "            with open(cfg['logfilename'], 'a+') as logfile:\n",
    "                logfile.write('sample {} of {} in epoch {} of {}.\\n'.format(i,\n",
    "                                                                            len(train_loader),\n",
    "                                                                            epoch,\n",
    "                                                                            cfg['epochs']))\n",
    "\n",
    "    #performance['loss'].append(np.mean(np.array(epoch_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10,64,450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([20, 64, 64])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    #print(np.shape(batch))\n",
    "    print(np.shape(batch[0]))\n",
    "    #print(batch[0][3][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"election on an anti-slavery platform, an initial seven slave states declared their secession from the country to form the Confederacy. War broke out in April 1861 \"\n",
    "        \n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "election on an anti-slavery platform, an initial seven slave states declared their secession from the country to form the Confederacy. War broke out in April 1861 "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64-len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(doc) <64:\n",
    "    txt = txt + ' PAD'*(63-len(doc))\n",
    "    doc = nlp(txt)\n",
    "elif len(doc) > 64:\n",
    "    doc = doc[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9114,\n",
       " 114,\n",
       " 117,\n",
       " 2873,\n",
       " 30,\n",
       " 7398,\n",
       " 1,\n",
       " 28,\n",
       " 117,\n",
       " 7540,\n",
       " 6888,\n",
       " 7323,\n",
       " 16878,\n",
       " 301,\n",
       " 191,\n",
       " 1,\n",
       " 455,\n",
       " 8,\n",
       " 5943,\n",
       " 6,\n",
       " 3110,\n",
       " 8,\n",
       " 1,\n",
       " 37,\n",
       " 4693,\n",
       " 6097,\n",
       " 220,\n",
       " 125,\n",
       " 14842,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election',\n",
       " 'on',\n",
       " 'an',\n",
       " 'anti',\n",
       " '-',\n",
       " 'slavery',\n",
       " 'platform',\n",
       " ',',\n",
       " 'an',\n",
       " 'initial',\n",
       " 'seven',\n",
       " 'slave',\n",
       " 'states',\n",
       " 'declared',\n",
       " 'their',\n",
       " 'secession',\n",
       " 'from',\n",
       " 'the',\n",
       " 'country',\n",
       " 'to',\n",
       " 'form',\n",
       " 'the',\n",
       " 'Confederacy',\n",
       " '.',\n",
       " 'War',\n",
       " 'broke',\n",
       " 'out',\n",
       " 'in',\n",
       " 'April',\n",
       " '1861',\n",
       " ' ',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PART',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'NUM',\n",
       " 'SPACE',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PROPN']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election',\n",
       " 'on',\n",
       " 'an',\n",
       " 'anti',\n",
       " '-',\n",
       " 'slavery',\n",
       " 'platform',\n",
       " ',',\n",
       " 'an',\n",
       " 'initial',\n",
       " 'seven',\n",
       " 'slave',\n",
       " 'state',\n",
       " 'declare',\n",
       " 'their',\n",
       " 'secession',\n",
       " 'from',\n",
       " 'the',\n",
       " 'country',\n",
       " 'to',\n",
       " 'form',\n",
       " 'the',\n",
       " 'Confederacy',\n",
       " '.',\n",
       " 'war',\n",
       " 'break',\n",
       " 'out',\n",
       " 'in',\n",
       " 'April',\n",
       " '1861',\n",
       " ' ',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, root_id = doc_to_adj(doc, directed=False, self_loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vocab.map([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats in equal dimension to input_ids\n",
    "rand = torch.rand(np.shape(vocab.map([token.text for token in doc])))\n",
    "# where the random array is less than 0.15, we set true\n",
    "mask_arr = rand < 0.15\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selection from mask_arr\n",
    "selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in selection: \n",
    "    inputs[i] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
