{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DaehanKim/vgae_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self, adj):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim, adj)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "\t\tself.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X,adj):\n",
    "\t\thidden = self.base_gcn(X,adj)\n",
    "\t\tself.mean = self.gcn_mean(hidden,adj)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden,adj)\n",
    "\t\tgaussian_noise = torch.randn(X.size(0), hidden2_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "\n",
    "\tdef forward(self, X,adj):\n",
    "\t\tZ = self.encode(X,adj)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.adj = adj\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tx = inputs\n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(self.adj, x)\n",
    "\t\toutputs = self.activation(x)\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "\tA_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "\treturn A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)\n",
    "\n",
    "\n",
    "class GAE(nn.Module):\n",
    "\tdef __init__(self,adj):\n",
    "\t\tsuper(GAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim, adj)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tz = self.mean = self.gcn_mean(hidden)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "model = 'VGAE'\n",
    "\n",
    "input_dim = 1433 \n",
    "hidden1_dim = 32\n",
    "hidden2_dim = 16\n",
    "use_feature = True\n",
    "\n",
    "num_epoch = 200\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "****************NOTE*****************\n",
    "CREDITS : Thomas Kipf\n",
    "since datasets are the same as those in kipf's implementation, \n",
    "Their preprocessing source was used as-is.\n",
    "*************************************\n",
    "'''\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "****************NOTE*****************\n",
    "CREDITS : Thomas Kipf\n",
    "since datasets are the same as those in kipf's implementation, \n",
    "Their preprocessing source was used as-is.\n",
    "*************************************\n",
    "'''\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "adj, features = load_data('cora')\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1433 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 9 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8976 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init model and optimizer\n",
    "model = GAE(adj_norm)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0001 train_loss= 0.47183 train_acc= 0.60303 val_roc= 0.90778 val_ap= 0.90528 time= 0.23836\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0002 train_loss= 0.47087 train_acc= 0.60307 val_roc= 0.90862 val_ap= 0.90647 time= 0.26330\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0003 train_loss= 0.46973 train_acc= 0.60324 val_roc= 0.90863 val_ap= 0.90607 time= 0.22639\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0004 train_loss= 0.46858 train_acc= 0.60348 val_roc= 0.90885 val_ap= 0.90619 time= 0.22639\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0005 train_loss= 0.46748 train_acc= 0.60356 val_roc= 0.90918 val_ap= 0.90708 time= 0.23038\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0006 train_loss= 0.46641 train_acc= 0.60363 val_roc= 0.90893 val_ap= 0.90660 time= 0.24834\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0007 train_loss= 0.46537 train_acc= 0.60334 val_roc= 0.90886 val_ap= 0.90593 time= 0.26429\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0008 train_loss= 0.46437 train_acc= 0.60288 val_roc= 0.90866 val_ap= 0.90564 time= 0.25731\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0009 train_loss= 0.46335 train_acc= 0.60239 val_roc= 0.90841 val_ap= 0.90464 time= 0.23836\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0010 train_loss= 0.46229 train_acc= 0.60214 val_roc= 0.90807 val_ap= 0.90389 time= 0.24435\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0011 train_loss= 0.46131 train_acc= 0.60204 val_roc= 0.90830 val_ap= 0.90323 time= 0.25033\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0012 train_loss= 0.46048 train_acc= 0.60187 val_roc= 0.90782 val_ap= 0.90208 time= 0.23537\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0013 train_loss= 0.45977 train_acc= 0.60169 val_roc= 0.90781 val_ap= 0.90139 time= 0.24834\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n",
      "Epoch: 0014 train_loss= 0.45906 train_acc= 0.60174 val_roc= 0.90788 val_ap= 0.90079 time= 0.23736\n",
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 2708])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c6475c69fdfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madj_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mval_roc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_edges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_edges_false\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-c6475c69fdfb>\u001b[0m in \u001b[0;36mget_acc\u001b[1;34m(adj_rec, adj_label)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mlabels_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpreds_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0madj_rec\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpreds_all\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlabels_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        # print(e)\n",
    "        # print(adj_rec[e[0], e[1]])\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy\n",
    "\n",
    "# train model\n",
    "for epoch in range(num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    print(np.shape(A_pred))\n",
    "    print(np.shape(adj_label))\n",
    "    optimizer.zero_grad()\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    if model == 'VGAE':\n",
    "        kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "        loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, filename='', load=False, threshold=0):\n",
    "        if load:\n",
    "            assert os.path.exists(filename), \"Vocab file does not exist at \" + filename\n",
    "\n",
    "            self.id2word, self.word2id = self.load(filename)\n",
    "            self.size = len(self.id2word)\n",
    "            self.threshold = threshold\n",
    "            self.wordCounter = None\n",
    "        else:\n",
    "            self.id2word, self.word2id = {}, {}\n",
    "            self.size = 0\n",
    "            self.threshold = threshold\n",
    "            # We always add some custom tokens into the vocabulary.\n",
    "            self.add_words(\n",
    "                {'<PAD>': float('inf'), '<UNK>': float('inf')})\n",
    "        self.word_embed = None\n",
    "\n",
    "    def add_words(self, counterOfTokens):\n",
    "        for item, value in counterOfTokens.items():\n",
    "            if value >= self.threshold:\n",
    "                if item not in self.word2id:\n",
    "                    # add it to the vocab\n",
    "                    self.word2id[item] = self.size\n",
    "                    self.id2word[self.size] = item\n",
    "                    self.size += 1\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as infile:\n",
    "            id2word = pickle.load(infile)\n",
    "            word2id = {word:id for id, word in id2word.items()}\n",
    "            self.id2word, self.word2id = id2word, word2id\n",
    "            self.size = len(self.id2word)\n",
    "\n",
    "        return id2word, word2id\n",
    "\n",
    "    def save(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "           \n",
    "        with open(filename, 'wb') as outfile:\n",
    "            pickle.dump(self.id2word, outfile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def init_word_embed(self, cfg, cache_dir='datasets/.word_vectors_cache'):\n",
    "        if cfg['word_vectors'] == 'Word2Vec':\n",
    "            from torchnlp.word_to_vector import FastText\n",
    "            all_word_vector = FastText(language=cfg['language'], cache=cache_dir, aligned=True)\n",
    "        else:\n",
    "            raise NotImplementedError('No word_vectors found which are called {}.'.format(cfg['word_vectors']))\n",
    "\n",
    "        # The the vectors only correspond to lower character words:\n",
    "        all_words = [word.lower() for word in list(self.word2id.keys())]\n",
    "        weights = all_word_vector[all_words]\n",
    "        \n",
    "        word_embed = torch.nn.Embedding(*weights.shape, _weight=weights)\n",
    "        if cfg['device'] == 'cuda':\n",
    "            word_embed.cuda()\n",
    "\n",
    "        self.word_embed = word_embed\n",
    "        self.embed_size = weights.shape[1]\n",
    "\n",
    "    def words2vecs(self, words: list):\n",
    "        if not self.word_embed:\n",
    "            raise AttributeError(\"The word embeddings aren't initialized yet.\")\n",
    "        else:\n",
    "            vecs = self.word_embed(torch.tensor(self.map(words), requires_grad=False))\n",
    "        return vecs\n",
    "\n",
    "    def one_hot_ids2vecs(self, ids):\n",
    "        vecs = self.word_embed(ids)\n",
    "        return vecs\n",
    "\n",
    "    def map(self, token_list):\n",
    "        \"\"\"\n",
    "        Map a list of tokens to their ids.\n",
    "        \"\"\"\n",
    "        return [self.word2id[w] if w in self.word2id else self.word2id['<UNK>'] for w in token_list]\n",
    "\n",
    "    def unmap(self, idx_list):\n",
    "        \"\"\"\n",
    "        Unmap ids back to tokens.\n",
    "        \"\"\"\n",
    "        return [self.id2word[idx] for idx in idx_list]\n",
    "    \n",
    "def get_pos_vocab():\n",
    "    \"\"\"\n",
    "    Function to set up a part of speech vocabulary handcrafed.\n",
    "    \"\"\"\n",
    "    pos_id2word = {0: '<PAD>', 1: '<UNK>', 2: 'DET', 3: 'PROPN', 4: 'VERB', 5: 'PART', 6: 'ADJ', 7: 'PUNCT', 8: 'CCONJ',\n",
    "                   9: 'ADP', 10: 'PRON', 11: 'NOUN', 12: 'ADV', 13: 'INTJ', 14: 'NUM', 15: 'X', 16: 'SYM'}\n",
    "    pos_word2id = {word: id for id, word in pos_id2word.items()}\n",
    "    pos_vocab = Vocab()\n",
    "    pos_vocab.id2word = pos_id2word\n",
    "    pos_vocab.word2id = pos_word2id\n",
    "    pos_vocab.size = len(pos_vocab.id2word)\n",
    "    \n",
    "    return pos_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load sst data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SSTData(Dataset):\n",
    "    def __init__(self,\n",
    "                 sst_data,\n",
    "                 vocab,\n",
    "                 nlp,\n",
    "                 lemma_vocab,\n",
    "                 pos_vocab = None,\n",
    "                 self_loop=True):\n",
    "\n",
    "        self.lemma_vocab = lemma_vocab\n",
    "        self.self_loop = self_loop\n",
    "        \n",
    "        \n",
    "        sst_data = [sample for sample in sst_data if sample['label'] != 'neutral']\n",
    "        self.sst_data = sst_data\n",
    "\n",
    "        self.sentiment_vocab = {'negative': 0, 'positive': 1}\n",
    "        \n",
    "        # Add sentencizer in the nlp if not already in it:\n",
    "        if \"sentencizer\" not in nlp.pipe_names:\n",
    "            # sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "            nlp.add_pipe('sentencizer', first=True)\n",
    "        self.nlp = nlp\n",
    "\n",
    "        self.vocab = vocab\n",
    "        if pos_vocab is None:\n",
    "            self.pos_vocab = get_pos_vocab()\n",
    "        else:\n",
    "            self.pos_vocab = pos_vocab\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.nlp(self.sst_data[idx]['text'])\n",
    "        # make lambda\n",
    "\n",
    "        adj, root_id = doc_to_adj(doc, directed=False, self_loop=self.self_loop)\n",
    "\n",
    "        lamb = adj\n",
    "\n",
    "        # normalize\n",
    "        denom = lamb.sum(1)\n",
    "        lamb /= denom\n",
    "\n",
    "        # make text indices\n",
    "        token_ids = self.vocab.map([token.text for token in doc])\n",
    "\n",
    "        # make pos ids\n",
    "        pos_ids = self.pos_vocab.map([token.pos_ for token in doc])\n",
    "\n",
    "        # make lemma ids\n",
    "        lemma_ids = self.lemma_vocab.map([token.lemma_ for token in doc])\n",
    "\n",
    "        # make label\n",
    "        label = self.sentiment_vocab[self.sst_data[idx]['label']]\n",
    "\n",
    "        return token_ids, pos_ids, lamb, label, root_id, lemma_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sst_data)\n",
    "\n",
    "\n",
    "def collate_fn_sentim(batch):\n",
    "    lens = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        lens.append(lamb.shape[1])\n",
    "\n",
    "    max_len = max(lens)\n",
    "    lambs = []\n",
    "    texts = []\n",
    "    poss = []\n",
    "    lemmas = []\n",
    "    labels = []\n",
    "    root_ids = []\n",
    "    for sample in batch:\n",
    "        text, pos, lamb, label, root_id, lemma = sample[:6]\n",
    "        # Big lamb\n",
    "        lamb_ = torch.zeros(1, max_len, max_len)\n",
    "        lamb_[0, :lamb.shape[0], :lamb.shape[1]] = lamb\n",
    "        lambs.append(lamb_)\n",
    "\n",
    "        # Big text\n",
    "        text_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        text_[0, :len(text)] = torch.tensor(text)\n",
    "        texts.append(text_)\n",
    "\n",
    "        # Big pos\n",
    "        pos_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        pos_[0, :len(pos)] = torch.tensor(pos)\n",
    "        poss.append(pos_)\n",
    "\n",
    "        # Big lemma\n",
    "        lemma_ = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        lemma_[0, :len(lemma)] = torch.tensor(lemma)\n",
    "        lemmas.append(lemma_)\n",
    "\n",
    "        # Big label:\n",
    "        label_ = torch.ones(1) * label\n",
    "        labels.append(label_.long())\n",
    "\n",
    "        # Big root_id:\n",
    "        root_id_ = torch.ones(1) * root_id\n",
    "        root_ids.append(root_id_.long())\n",
    "\n",
    "    lambs = torch.cat(lambs, dim=0)\n",
    "    texts = torch.cat(texts, dim=0)\n",
    "    poss = torch.cat(poss, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    root_ids = torch.cat(root_ids, dim=0)\n",
    "    lemmas = torch.cat(lemmas, dim=0)\n",
    "\n",
    "    return lambs, poss, texts, labels, lens, root_ids, lemmas\n",
    "\n",
    "\n",
    "def doc_to_adj(sent, directed=True, self_loop=False):\n",
    "    # Sent should be a spacy document. Can also be longer than a sentence.\n",
    "    sent_len = len(sent)\n",
    "\n",
    "    ret = torch.zeros(sent_len, sent_len, dtype=torch.float32)\n",
    "\n",
    "    for token in sent:\n",
    "        for child in token.children:\n",
    "            if child.i >= sent_len:\n",
    "                print('Something goes wrong here.')\n",
    "                print(child.i, sent_len, sent, token.i, token)\n",
    "            ret[token.i, child.i] = 1\n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_id = token.i\n",
    "\n",
    "    if not directed:\n",
    "        ret = ret + ret.transpose(0, 1)\n",
    "\n",
    "    if self_loop:\n",
    "        for i in range(sent_len):\n",
    "            ret[i, i] = 1\n",
    "\n",
    "    return ret, root_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
