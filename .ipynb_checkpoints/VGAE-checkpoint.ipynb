{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self, adj):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)\n",
    "\t\tself.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)\n",
    "\t\tself.gcn_logstddev = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tself.mean = self.gcn_mean(hidden)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden)\n",
    "\t\tgaussian_noise = torch.randn(X.size(0), args.hidden2_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.adj = adj\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tx = inputs\n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(self.adj, x)\n",
    "\t\toutputs = self.activation(x)\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "\tA_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "\treturn A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)\n",
    "\n",
    "\n",
    "class GAE(nn.Module):\n",
    "\tdef __init__(self,adj):\n",
    "\t\tsuper(GAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)\n",
    "\t\tself.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tz = self.mean = self.gcn_mean(hidden)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "model = 'GAE'\n",
    "\n",
    "input_dim = 1433 \n",
    "hidden1_dim = 32\n",
    "hidden2_dim = 16\n",
    "use_feature = True\n",
    "\n",
    "num_epoch = 200\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "****************NOTE*****************\n",
    "CREDITS : Thomas Kipf\n",
    "since datasets are the same as those in kipf's implementation, \n",
    "Their preprocessing source was used as-is.\n",
    "*************************************\n",
    "'''\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "****************NOTE*****************\n",
    "CREDITS : Thomas Kipf\n",
    "since datasets are the same as those in kipf's implementation, \n",
    "Their preprocessing source was used as-is.\n",
    "*************************************\n",
    "'''\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from input_data import load_data\n",
    "from preprocessing import *\n",
    "import args\n",
    "import model\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "adj, features = load_data('cora')\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "# init model and optimizer\n",
    "model = getattr(model,args.model)(adj_norm)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        # print(e)\n",
    "        # print(adj_rec[e[0], e[1]])\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy\n",
    "\n",
    "# train model\n",
    "for epoch in range(args.num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    if args.model == 'VGAE':\n",
    "        kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "        loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
