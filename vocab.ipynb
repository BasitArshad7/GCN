{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Path('datasets/wikitext-103/wiki.train.tokens').read_text(encoding='utf-8')\n",
    "val_data = Path('datasets/wikitext-103/wiki.valid.tokens').read_text(encoding='utf-8')\n",
    "test_data = Path('datasets/wikitext-103/wiki.test.tokens').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out train headings and articles\n",
    "train_split = re.split(heading_pattern, train_data)\n",
    "train_headings = [x[7:-7] for x in train_split[1::2]]\n",
    "train_articles = [x for x in train_split[2::2]]\n",
    "\n",
    "# Split out validation headings and articles\n",
    "val_split = re.split(heading_pattern, val_data)\n",
    "val_headings = [x[7:-7] for x in val_split[1::2]]\n",
    "val_articles = [x for x in val_split[2::2]]\n",
    "\n",
    "# Split out test headings and articles\n",
    "test_split = re.split(heading_pattern, test_data)\n",
    "test_headings = [x[7:-7] for x in test_split[1::2]]\n",
    "test_articles = [x for x in test_split[2::2]]\n",
    "\n",
    "\n",
    "train_data = [i.split('. ') for i in train_articles]\n",
    "val_data = [i.split('. ') for i in val_articles]\n",
    "test_data = [i.split('. ') for i in test_articles]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [item for sublist in test_data for item in sublist]\n",
    "val_data = [item for sublist in val_data for item in sublist]\n",
    "train_data = [item for sublist in train_data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "cw = Counter()\n",
    "cl = Counter()\n",
    "\n",
    "# Count words:\n",
    "for sample in train_set + test_set: cw += Counter([token.text for token in nlp(sample)])\n",
    "# Count lemma:\n",
    "for sample in train_set + test_set: cl += Counter([token.lemma_ for token in nlp(sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, filename='', load=False, threshold=500):\n",
    "        if load:\n",
    "            assert os.path.exists(filename), \"Vocab file does not exist at \" + filename\n",
    "\n",
    "            self.id2word, self.word2id = self.load(filename)\n",
    "            self.size = len(self.id2word)\n",
    "            self.threshold = threshold\n",
    "            self.wordCounter = None\n",
    "        else:\n",
    "            self.id2word, self.word2id = {}, {}\n",
    "            self.size = 0\n",
    "            self.threshold = threshold\n",
    "            # We always add some custom tokens into the vocabulary.\n",
    "            self.add_words(\n",
    "                {'<PAD>': float('inf'), '<UNK>': float('inf'),'<MSK>' : 103})\n",
    "        self.word_embed = None\n",
    "\n",
    "    def add_words(self, counterOfTokens):\n",
    "        for item, value in counterOfTokens.items():\n",
    "            if value >= self.threshold:\n",
    "                if item not in self.word2id:\n",
    "                    # add it to the vocab\n",
    "                    self.word2id[item] = self.size\n",
    "                    self.id2word[self.size] = item\n",
    "                    self.size += 1\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as infile:\n",
    "            id2word = pickle.load(infile)\n",
    "            word2id = {word:id for id, word in id2word.items()}\n",
    "            self.id2word, self.word2id = id2word, word2id\n",
    "            self.size = len(self.id2word)\n",
    "\n",
    "        return id2word, word2id\n",
    "\n",
    "    def save(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "           \n",
    "        with open(filename, 'wb') as outfile:\n",
    "            pickle.dump(self.id2word, outfile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def init_word_embed(self, cfg, cache_dir='datasets/.word_vectors_cache'):\n",
    "        if cfg['word_vectors'] == 'Word2Vec':\n",
    "            from torchnlp.word_to_vector import FastText\n",
    "            all_word_vector = FastText(language=cfg['language'], cache=cache_dir, aligned=True)\n",
    "        else:\n",
    "            raise NotImplementedError('No word_vectors found which are called {}.'.format(cfg['word_vectors']))\n",
    "\n",
    "        # The the vectors only correspond to lower character words:\n",
    "        all_words = [word.lower() for word in list(self.word2id.keys())]\n",
    "        weights = all_word_vector[all_words]\n",
    "        \n",
    "        word_embed = torch.nn.Embedding(*weights.shape, _weight=weights)\n",
    "        #if cfg['device'] == 'cuda':\n",
    "        #    word_embed.cuda()\n",
    "\n",
    "        self.word_embed = word_embed\n",
    "        self.embed_size = weights.shape[1]\n",
    "\n",
    "    def words2vecs(self, words: list):\n",
    "        if not self.word_embed:\n",
    "            raise AttributeError(\"The word embeddings aren't initialized yet.\")\n",
    "        else:\n",
    "            vecs = self.word_embed(torch.tensor(self.map(words), requires_grad=False))\n",
    "        return vecs\n",
    "\n",
    "    def one_hot_ids2vecs(self, ids):\n",
    "        vecs = self.word_embed(ids)\n",
    "        return vecs\n",
    "\n",
    "    def map(self, token_list):\n",
    "        \"\"\"\n",
    "        Map a list of tokens to their ids.\n",
    "        \"\"\"\n",
    "        return [self.word2id[w] if w in self.word2id else self.word2id['<UNK>'] for w in token_list]\n",
    "\n",
    "    def unmap(self, idx_list):\n",
    "        \"\"\"\n",
    "        Unmap ids back to tokens.\n",
    "        \"\"\"\n",
    "        return [self.id2word[idx] for idx in idx_list]\n",
    "    \n",
    "def get_pos_vocab():\n",
    "    \"\"\"\n",
    "    Function to set up a part of speech vocabulary handcrafed.\n",
    "    \"\"\"\n",
    "    pos_id2word = {0: '<PAD>', 1: '<UNK>', 2: 'DET', 3: 'PROPN', 4: 'VERB', 5: 'PART', 6: 'ADJ', 7: 'PUNCT', 8: 'CCONJ',\n",
    "                   9: 'ADP', 10: 'PRON', 11: 'NOUN', 12: 'ADV', 13: 'INTJ', 14: 'NUM', 15: 'X', 16: 'SYM'}\n",
    "    pos_word2id = {word: id for id, word in pos_id2word.items()}\n",
    "    pos_vocab = Vocab()\n",
    "    pos_vocab.id2word = pos_id2word\n",
    "    pos_vocab.word2id = pos_word2id\n",
    "    pos_vocab.size = len(pos_vocab.id2word)\n",
    "    \n",
    "    return pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "lemma_vocab = Vocab()\n",
    "\n",
    "\n",
    "# prepare vocab\n",
    "vocab.add_words(cw)\n",
    "#cfg['input_dimension'] = 300\n",
    "\n",
    "lemma_vocab.add_words(cl)\n",
    "pos_vocab = get_pos_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13234"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save('word_vocab_500.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_vocab.save('lemma_vocab_500.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vocab.save('pos_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
